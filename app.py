import os
import json
import numpy as np
import pandas as pd
from flask import Flask, render_template, request, jsonify, send_from_directory
import librosa
import scipy.stats
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
import parselmouth
from parselmouth.praat import call
import speech_recognition as sr
from pydub import AudioSegment
import tempfile
import warnings
warnings.filterwarnings('ignore')
from datetime import datetime
import scipy.signal
from scipy.signal import butter, filtfilt

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size

class AlzheimersVoiceAnalyzer:
    def __init__(self):
        self.scaler = StandardScaler()
        self.model = None
        self.feature_names = []
        self.speech_tasks = self._initialize_speech_tasks()
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the ML model with research-based features"""
        # For demo purposes, we'll use a pre-configured model
        # In production, this would be trained on clinical datasets
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        
        # Define feature names based on research
        self.feature_names = [
            'speaking_rate', 'pause_rate', 'pause_duration_mean', 'pause_duration_std',
            'phonation_time', 'speech_time', 'articulation_rate', 'voice_breaks',
            'f0_mean', 'f0_std', 'f0_range', 'jitter', 'shimmer', 'hnr',
            'spectral_centroid', 'spectral_rolloff', 'mfcc_mean', 'mfcc_std',
            'silence_ratio', 'voiceless_ratio', 'word_count', 'unique_words',
            'type_token_ratio', 'avg_word_length', 'hesitation_count', 'repetition_count',
            'semantic_fluency', 'syntactic_complexity', 'idea_density', 'coherence_score',
            'task_performance_score', 'element_coverage', 'information_density'
        ]
        
        # Create more realistic training data for demonstration
        # In production, replace with real clinical data
        np.random.seed(42)
        n_samples = 1000
        
        # Generate more realistic feature distributions
        X_dummy = np.random.randn(n_samples, len(self.feature_names))
        
        # Moderate distribution for better sensitivity
        y_dummy = np.random.choice([0, 1], n_samples, p=[0.78, 0.22])  # 78% healthy, 22% AD
        
        # Adjust features to reflect more realistic AD patterns
        for i, label in enumerate(y_dummy):
            if label == 1:  # AD patient - more pronounced differences
                X_dummy[i, 0] *= 0.5  # Much lower speaking rate
                X_dummy[i, 1] += 1.0  # Much higher pause rate
                X_dummy[i, 7] += 0.8  # More voice breaks
                X_dummy[i, 22] *= 0.4  # Much lower type-token ratio
                X_dummy[i, 24] += 0.7  # More hesitations
                X_dummy[i, 25] += 0.5  # More repetitions
            else:  # Healthy controls - keep closer to normal
                X_dummy[i, 0] = abs(X_dummy[i, 0]) + 0.5  # Ensure positive speaking rate
                X_dummy[i, 1] = abs(X_dummy[i, 1]) * 0.3  # Lower pause rate
                X_dummy[i, 22] = abs(X_dummy[i, 22]) * 0.3 + 0.4  # Good type-token ratio
        
        self.scaler.fit(X_dummy)
        self.model.fit(X_dummy, y_dummy)
    
    def _initialize_speech_tasks(self):
        """Initialize 50 speech prompts across 5 task categories (10 per category)"""
        import random
        
        return {
            'picture_description': {
                'name': 'Picture Description',
                'prompts': [
                    'Describe what you see in this kitchen scene. Include all the people, objects, and activities.',
                    'Tell me about this park scene. What are the people doing? What objects do you notice?',
                    'Look at this family gathering photo. Describe the people, their expressions, and the setting.',
                    'Describe this busy street scene. What vehicles, buildings, and people can you see?',
                    'Tell me about this beach scene. What activities and objects are visible?',
                    'Describe this classroom setting. What are the students and teacher doing?',
                    'Look at this garden scene. Describe the plants, tools, and any people you see.',
                    'Tell me about this living room. Describe the furniture, decorations, and any people.',
                    'Describe this hospital waiting room. What do you notice about the people and environment?',
                    'Look at this restaurant scene. Describe the diners, staff, and setting.'
                ],
                'expected_elements': ['people', 'objects', 'actions', 'colors', 'locations', 'emotions', 'details'],
                'target_duration': 60,
                'analysis_focus': ['semantic_access', 'visual_processing', 'descriptive_language']
            },
            'semantic_fluency': {
                'name': 'Semantic Fluency',
                'prompts': [
                    'Name as many animals as you can in one minute. Start now.',
                    'Tell me all the fruits you can think of. You have one minute.',
                    'List as many types of vehicles as possible in one minute.',
                    'Name all the clothing items you can think of. One minute starting now.',
                    'Tell me as many kitchen items as you can in one minute.',
                    'List all the colors you can name in one minute.',
                    'Name as many sports as you can think of. One minute timer starts now.',
                    'Tell me all the body parts you can name in one minute.',
                    'List as many professions or jobs as possible in one minute.',
                    'Name all the musical instruments you can think of. One minute starting now.'
                ],
                'expected_elements': ['category_items', 'semantic_clustering', 'fluency_rate'],
                'target_duration': 60,
                'analysis_focus': ['semantic_fluency', 'category_access', 'cognitive_flexibility']
            },
            'story_retelling': {
                'name': 'Story Retelling',
                'prompts': [
                    'Listen to this story and retell it: "A woman was baking cookies when she realized she was out of sugar. She asked her neighbor for some, but they were also out. She decided to go to the store, but her car wouldn\'t start. Finally, she walked to the store and bought sugar." Now retell this story.',
                    'Here\'s a story to retell: "A man was reading in the park when it started to rain. He ran to a nearby caf√© for shelter. There he met an old friend from school. They talked for hours until the rain stopped." Please retell this story.',
                    'Listen and retell: "A little girl lost her favorite toy at the playground. She looked everywhere but couldn\'t find it. Her mother helped her search. Finally, they found it under the slide where another child had been playing." Retell this story.',
                    'Story to retell: "An elderly man was feeding birds in his garden every morning. One day, a injured bird couldn\'t fly away. He carefully took it inside, cared for it, and released it when it healed." Please retell this.',
                    'Listen to this: "A student forgot her lunch at home. Her friend offered to share, but she was too shy to accept. By afternoon, she was very hungry. Finally, her friend insisted on sharing her sandwich." Retell this story.',
                    'Here\'s the story: "A family planned a picnic, but it rained. They decided to have an indoor picnic instead. They spread blankets in the living room and had a wonderful time." Please retell this.',
                    'Story to retell: "A dog escaped from its yard and got lost. A kind stranger found it and checked its collar for an address. The stranger walked the dog home to its worried family." Retell this story.',
                    'Listen carefully: "A baker arrived early to find his shop\'s window broken. Nothing was stolen, just broken glass everywhere. He cleaned up, fixed the window, and opened on time." Please retell this.',
                    'Here\'s the story: "Two friends planned to meet at a movie theater. One got stuck in traffic and arrived late. The movie had already started, so they decided to see the next showing instead." Retell this.',
                    'Story to retell: "A librarian noticed books were being returned damaged. She investigated and found children were eating while reading. She created a no-food policy and the problem stopped." Please retell this.'
                ],
                'expected_elements': ['main_characters', 'key_events', 'sequence', 'outcome'],
                'target_duration': 45,
                'analysis_focus': ['memory_recall', 'narrative_structure', 'sequential_processing']
            },
            'procedural_description': {
                'name': 'Procedural Description',
                'prompts': [
                    'Explain how to make a peanut butter and jelly sandwich, step by step.',
                    'Describe how to brush your teeth properly, from start to finish.',
                    'Tell me how to make a cup of coffee or tea, including all the steps.',
                    'Explain how to tie your shoelaces, step by step.',
                    'Describe how to wash dishes by hand, from beginning to end.',
                    'Tell me how to make scrambled eggs, including all the steps.',
                    'Explain how to wrap a gift, step by step.',
                    'Describe how to plant a seed in a pot, from start to finish.',
                    'Tell me how to change a light bulb safely, including all steps.',
                    'Explain how to make a paper airplane, step by step.'
                ],
                'expected_elements': ['sequential_steps', 'materials_needed', 'actions', 'order'],
                'target_duration': 90,
                'analysis_focus': ['procedural_knowledge', 'sequential_planning', 'executive_function']
            },
            'spontaneous_speech': {
                'name': 'Spontaneous Speech',
                'prompts': [
                    'Tell me about your typical daily routine from morning to evening.',
                    'Describe your favorite hobby or activity and why you enjoy it.',
                    'Tell me about a memorable vacation or trip you\'ve taken.',
                    'Describe your childhood home and neighborhood.',
                    'Tell me about your family and what they mean to you.',
                    'Describe your favorite season and what you like about it.',
                    'Tell me about a skill or talent you have and how you developed it.',
                    'Describe your ideal weekend and how you would spend it.',
                    'Tell me about a book, movie, or TV show you really enjoyed.',
                    'Describe a challenge you\'ve overcome and how you did it.'
                ],
                'expected_elements': ['personal_details', 'temporal_organization', 'coherent_narrative'],
                'target_duration': 120,
                'analysis_focus': ['discourse_coherence', 'topic_maintenance', 'spontaneous_organization']
            }
        }
    
    def extract_acoustic_features(self, audio_path):
        """Extract acoustic features using research-based methods"""
        try:
            # Load audio with comprehensive format support
            try:
                # Try librosa first (preferred for audio analysis)
                y, sr = librosa.load(audio_path, sr=None)
                print(f"Loaded audio with librosa: duration={len(y)/sr:.2f}s, sr={sr}Hz")
            except Exception as e:
                print(f"Librosa failed: {e}")
                try:
                    # Fallback to pydub for broader format support (without ffmpeg dependency)
                    audio = AudioSegment.from_wav(audio_path)  # Try WAV first
                    # Convert to numpy array
                    samples = np.array(audio.get_array_of_samples())
                    if audio.channels == 2:
                        samples = samples.reshape((-1, 2)).mean(axis=1)  # Convert stereo to mono
                    
                    # Normalize to [-1, 1] range
                    if audio.sample_width == 2:  # 16-bit
                        samples = samples / 32768.0
                    elif audio.sample_width == 4:  # 32-bit
                        samples = samples / 2147483648.0
                    else:  # 8-bit or other
                        samples = samples / 128.0
                    
                    y = samples.astype(np.float32)
                    sr = audio.frame_rate
                    print(f"Loaded audio with pydub: duration={len(y)/sr:.2f}s, sr={sr}Hz")
                except Exception as e2:
                    print(f"Both librosa and pydub failed: librosa={e}, pydub={e2}")
                    # Create dummy data to prevent complete failure
                    print("Creating dummy audio data for analysis...")
                    y = np.random.normal(0, 0.1, 16000)  # 1 second of dummy audio
                    sr = 16000
                    print("Using dummy audio data - analysis will be limited")
            
            duration = len(y) / sr
            print(f"Audio loaded: duration={duration:.2f}s, sample_rate={sr}")
            
            # Apply noise reduction and audio enhancement
            y = self._enhance_audio_quality(y, sr)
            
            # Check minimum duration
            if duration < 0.5:
                print("Warning: Audio too short, using default values")
                return {feature: 0.1 for feature in self.feature_names[:20]}
            
            # Basic timing features with error handling
            speaking_rate = self._calculate_speaking_rate(y, sr)
            pause_features = self._analyze_pauses(y, sr)
            
            # Voice quality features with Praat
            f0_mean = f0_std = f0_range = jitter = shimmer = hnr = 0
            try:
                sound = parselmouth.Sound(audio_path)
                
                # Pitch analysis
                pitch = sound.to_pitch()
                f0_values = pitch.selected_array['frequency']
                f0_values = f0_values[f0_values != 0]  # Remove unvoiced frames
                
                if len(f0_values) > 0:
                    f0_mean = np.mean(f0_values)
                    f0_std = np.std(f0_values)
                    f0_range = np.ptp(f0_values)
                
                # Jitter and shimmer (voice quality)
                try:
                    jitter = call(sound, "Get jitter (local)", 0, 0, 0.0001, 0.02, 1.3)
                    shimmer = call(sound, "Get shimmer (local)", 0, 0, 0.0001, 0.02, 1.3, 1.6)
                except:
                    jitter = shimmer = 0.01  # Default small values
                
                # Harmonics-to-noise ratio
                try:
                    harmonicity = sound.to_harmonicity()
                    hnr = call(harmonicity, "Get mean", 0, 0)
                except:
                    hnr = 10  # Default reasonable value
                    
            except Exception as e:
                print(f"Praat analysis failed: {e}, using defaults")
                f0_mean, f0_std, f0_range = 150, 20, 100  # Default values
                jitter, shimmer, hnr = 0.01, 0.05, 10
            
            # Spectral features with error handling
            try:
                spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
                spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
                spectral_centroid_mean = np.mean(spectral_centroids)
                spectral_rolloff_mean = np.mean(spectral_rolloff)
            except:
                spectral_centroid_mean = 2000  # Default value
                spectral_rolloff_mean = 4000   # Default value
            
            # MFCC features with error handling
            try:
                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
                mfcc_mean = np.mean(mfccs)
                mfcc_std = np.std(mfccs)
            except:
                mfcc_mean, mfcc_std = 0, 1  # Default values
            
            # Voice activity detection
            voice_activity = self._detect_voice_activity(y, sr)
            silence_ratio = 1 - np.mean(voice_activity)
            
            return {
                'speaking_rate': max(speaking_rate, 10),  # Ensure minimum value
                'pause_rate': pause_features['rate'],
                'pause_duration_mean': pause_features['duration_mean'],
                'pause_duration_std': pause_features['duration_std'],
                'phonation_time': duration * (1 - silence_ratio),
                'speech_time': duration,
                'articulation_rate': speaking_rate / (1 - silence_ratio) if silence_ratio < 0.9 else speaking_rate,
                'voice_breaks': self._count_voice_breaks(y, sr),
                'f0_mean': f0_mean,
                'f0_std': f0_std,
                'f0_range': f0_range,
                'jitter': jitter,
                'shimmer': shimmer,
                'hnr': hnr,
                'spectral_centroid': spectral_centroid_mean,
                'spectral_rolloff': spectral_rolloff_mean,
                'mfcc_mean': mfcc_mean,
                'mfcc_std': mfcc_std,
                'silence_ratio': min(silence_ratio, 0.8),  # Cap silence ratio
                'voiceless_ratio': self._calculate_voiceless_ratio(y, sr)
            }
        except Exception as e:
            print(f"Error extracting acoustic features: {e}")
            import traceback
            traceback.print_exc()
            # Return default values instead of empty dict
            return {
                'speaking_rate': 100, 'pause_rate': 0.2, 'pause_duration_mean': 0.5, 'pause_duration_std': 0.2,
                'phonation_time': 5, 'speech_time': 6, 'articulation_rate': 120, 'voice_breaks': 1,
                'f0_mean': 150, 'f0_std': 20, 'f0_range': 100, 'jitter': 0.01, 'shimmer': 0.05, 'hnr': 10,
                'spectral_centroid': 2000, 'spectral_rolloff': 4000, 'mfcc_mean': 0, 'mfcc_std': 1,
                'silence_ratio': 0.3, 'voiceless_ratio': 0.2
            }
    
    def extract_linguistic_features(self, transcript):
        """Extract linguistic features from transcript"""
        if not transcript:
            return {feature: 0 for feature in self.feature_names[20:]}
        
        words = transcript.lower().split()
        unique_words = set(words)
        
        # Basic lexical features
        word_count = len(words)
        unique_word_count = len(unique_words)
        type_token_ratio = unique_word_count / word_count if word_count > 0 else 0
        avg_word_length = np.mean([len(word) for word in words]) if words else 0
        
        # Enhanced hesitation and repetition detection
        hesitation_markers = ['uh', 'um', 'er', 'ah', 'well', 'like', 'you know', 'so', 'actually', 'basically', 'literally']
        hesitation_count = sum(1 for word in words if any(marker in word for marker in hesitation_markers))
        
        # Improved repetition detection - consecutive word repetitions
        repetition_count = 0
        for i in range(len(words) - 1):
            if words[i] == words[i + 1]:
                repetition_count += 1
        
        # Add phrase repetitions
        for i in range(len(words) - 2):
            two_word_phrase = f"{words[i]} {words[i+1]}"
            for j in range(i + 2, len(words) - 1):
                if f"{words[j]} {words[j+1]}" == two_word_phrase:
                    repetition_count += 1
        
        # Simplified semantic and syntactic measures
        semantic_fluency = self._estimate_semantic_fluency(words)
        syntactic_complexity = self._estimate_syntactic_complexity(transcript)
        idea_density = unique_word_count / len(transcript.split('.')) if '.' in transcript else unique_word_count
        coherence_score = self._estimate_coherence(words)
        
        return {
            'word_count': word_count,
            'unique_words': unique_word_count,
            'type_token_ratio': type_token_ratio,
            'avg_word_length': avg_word_length,
            'hesitation_count': hesitation_count,
            'repetition_count': repetition_count,
            'semantic_fluency': semantic_fluency,
            'syntactic_complexity': syntactic_complexity,
            'idea_density': idea_density,
            'coherence_score': coherence_score
        }
    
    def _calculate_speaking_rate(self, y, sr):
        """Calculate speaking rate with improved voice activity detection"""
        try:
            # Enhanced voice activity detection
            frame_length = int(0.025 * sr)  # 25ms frames
            hop_length = int(0.01 * sr)     # 10ms hop
            
            # Multi-feature voice activity detection
            energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
            zcr = librosa.feature.zero_crossing_rate(y, frame_length=frame_length, hop_length=hop_length)[0]
            
            # Normalize features
            energy_norm = (energy - np.min(energy)) / (np.max(energy) - np.min(energy) + 1e-8)
            zcr_norm = (zcr - np.min(zcr)) / (np.max(zcr) - np.min(zcr) + 1e-8)
            
            # Adaptive thresholding for voice activity
            energy_threshold = np.percentile(energy_norm, 25)  # More sensitive threshold
            zcr_threshold = np.percentile(zcr_norm, 75)
            
            # Voice activity: high energy AND low zero-crossing rate (characteristic of speech)
            voiced_frames = (energy_norm > energy_threshold) & (zcr_norm < zcr_threshold)
            
            # Calculate speech segments
            speech_segments = self._get_speech_segments(voiced_frames, hop_length, sr)
            
            if len(speech_segments) > 0:
                total_speech_time = sum(end - start for start, end in speech_segments)
                
                # Estimate syllables using energy peaks in speech segments
                syllable_count = self._estimate_syllables_from_audio(y, sr, speech_segments)
                
                # Convert syllables to words (average 1.3 syllables per word in English)
                estimated_words = max(1, syllable_count / 1.3)
                
                # Calculate speaking rate
                if total_speech_time > 0:
                    speaking_rate = (estimated_words / total_speech_time) * 60
                    return max(min(speaking_rate, 300), 30)  # Clamp between 30-300 WPM
                else:
                    return 120
            else:
                return 60  # Very slow if no clear speech detected
                
        except Exception as e:
            print(f"Speaking rate calculation failed: {e}")
            return 120
    
    def _get_speech_segments(self, voiced_frames, hop_length, sr):
        """Extract continuous speech segments"""
        segments = []
        in_speech = False
        start_frame = 0
        
        for i, is_voiced in enumerate(voiced_frames):
            if is_voiced and not in_speech:
                start_frame = i
                in_speech = True
            elif not is_voiced and in_speech:
                # End of speech segment
                start_time = start_frame * hop_length / sr
                end_time = i * hop_length / sr
                if end_time - start_time > 0.1:  # Minimum 100ms segment
                    segments.append((start_time, end_time))
                in_speech = False
        
        # Handle case where speech continues to end
        if in_speech:
            start_time = start_frame * hop_length / sr
            end_time = len(voiced_frames) * hop_length / sr
            if end_time - start_time > 0.1:
                segments.append((start_time, end_time))
        
        return segments
    
    def _estimate_syllables_from_audio(self, y, sr, speech_segments):
        """Estimate syllable count from audio energy peaks"""
        syllable_count = 0
        
        for start_time, end_time in speech_segments:
            start_sample = int(start_time * sr)
            end_sample = int(end_time * sr)
            segment = y[start_sample:end_sample]
            
            if len(segment) > 0:
                # Calculate energy envelope
                frame_length = int(0.02 * sr)  # 20ms frames
                hop_length = int(0.01 * sr)    # 10ms hop
                
                energy = librosa.feature.rms(y=segment, frame_length=frame_length, hop_length=hop_length)[0]
                
                # Find peaks (potential syllable nuclei)
                try:
                    from scipy.signal import find_peaks
                    peaks, _ = find_peaks(energy, 
                                        height=np.percentile(energy, 30),
                                        distance=int(0.1 * sr / hop_length))  # Min 100ms between syllables
                    syllable_count += len(peaks)
                except:
                    # Fallback: estimate based on segment duration
                    syllable_count += max(1, int((end_time - start_time) * 3))  # ~3 syllables per second
        
        return max(syllable_count, 1)

    def _analyze_pauses(self, y, sr):
        """Enhanced pause analysis with better detection"""
        try:
            frame_length = int(0.025 * sr)  # 25ms frames
            hop_length = int(0.01 * sr)     # 10ms hop
            
            # Multi-feature pause detection
            energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
            zcr = librosa.feature.zero_crossing_rate(y, frame_length=frame_length, hop_length=hop_length)[0]
            
            # Adaptive threshold based on signal characteristics
            energy_threshold = np.percentile(energy, 15)  # Lower percentile for pause detection
            zcr_threshold = np.percentile(zcr, 85)        # High ZCR often indicates silence/noise
            
            # Pause detection: low energy OR high zero-crossing rate
            pauses = (energy < energy_threshold) | (zcr > zcr_threshold)
            
            # Smooth pause detection to avoid micro-pauses
            try:
                from scipy import ndimage
                pauses = ndimage.binary_closing(pauses, structure=np.ones(3))  # Fill small gaps
                pauses = ndimage.binary_opening(pauses, structure=np.ones(5))   # Remove tiny pauses
            except:
                pass
            
            # Extract pause segments
            pause_segments = []
            in_pause = False
            pause_start = 0
            
            for i, is_pause in enumerate(pauses):
                if is_pause and not in_pause:
                    pause_start = i
                    in_pause = True
                elif not is_pause and in_pause:
                    pause_duration = (i - pause_start) * hop_length / sr
                    if pause_duration > 0.15:  # Minimum 150ms pause (more realistic)
                        pause_segments.append(pause_duration)
                    in_pause = False
            
            # Handle final pause
            if in_pause:
                pause_duration = (len(pauses) - pause_start) * hop_length / sr
                if pause_duration > 0.15:
                    pause_segments.append(pause_duration)
            
            if not pause_segments:
                return {'rate': 0, 'duration_mean': 0, 'duration_std': 0}
            
            total_duration = len(y) / sr
            pause_rate = len(pause_segments) / total_duration if total_duration > 0 else 0
            
            return {
                'rate': pause_rate,
                'duration_mean': np.mean(pause_segments),
                'duration_std': np.std(pause_segments) if len(pause_segments) > 1 else 0
            }
            
        except Exception as e:
            print(f"Pause analysis failed: {e}")
            return {'rate': 0.2, 'duration_mean': 0.5, 'duration_std': 0.2}
    
    def _detect_voice_activity(self, y, sr):
        """Simple voice activity detection"""
        frame_length = int(0.025 * sr)
        hop_length = int(0.01 * sr)
        
        energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
        threshold = np.mean(energy) * 0.2
        
        return energy > threshold
    
    def _count_voice_breaks(self, y, sr):
        """Count voice breaks (interruptions in voicing)"""
        voice_activity = self._detect_voice_activity(y, sr)
        breaks = 0
        was_voiced = False
        
        for is_voiced in voice_activity:
            if was_voiced and not is_voiced:
                breaks += 1
            was_voiced = is_voiced
        
        return breaks / (len(y) / sr)  # Normalize by duration
    
    def _calculate_voiceless_ratio(self, y, sr):
        """Calculate ratio of voiceless segments"""
        voice_activity = self._detect_voice_activity(y, sr)
        return 1 - np.mean(voice_activity)
    
    def _estimate_semantic_fluency(self, words):
        """Simplified semantic fluency estimation"""
        # Count content words vs function words
        function_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall'}
        content_words = [w for w in words if w not in function_words]
        return len(content_words) / len(words) if words else 0
    
    def _estimate_syntactic_complexity(self, transcript):
        """Simplified syntactic complexity estimation"""
        sentences = transcript.split('.')
        avg_sentence_length = np.mean([len(s.split()) for s in sentences if s.strip()])
        return avg_sentence_length if not np.isnan(avg_sentence_length) else 0
    
    def _estimate_coherence(self, words):
        """Simplified coherence estimation"""
        if len(words) < 2:
            return 0
        
        # Simple measure based on word repetition and flow
        unique_ratio = len(set(words)) / len(words)
        return unique_ratio
    
    def _get_timing_interpretation(self, status):
        interpretations = {
            'Normal': 'Speech rate and pausing patterns within expected range',
            'Concerning': 'Mild alterations in speech timing observed',
            'Impaired': 'Notable disruptions in speech flow and timing',
            'Severely Impaired': 'Significant speech timing abnormalities consistent with cognitive decline'
        }
        return interpretations.get(status, 'Unknown')
    
    def _get_voice_interpretation(self, status):
        interpretations = {
            'Normal': 'Voice quality parameters within normal limits',
            'Concerning': 'Mild voice quality changes noted',
            'Abnormal': 'Voice instability and quality degradation present',
            'Significantly Abnormal': 'Marked voice quality deterioration suggesting neurological involvement'
        }
        return interpretations.get(status, 'Unknown')
    
    def _get_lexical_interpretation(self, status):
        interpretations = {
            'Normal': 'Vocabulary use and semantic access appear intact',
            'Concerning': 'Mild reduction in lexical diversity observed',
            'Reduced': 'Notable limitations in vocabulary access and semantic fluency',
            'Severely Reduced': 'Significant lexical-semantic deficits consistent with cognitive impairment'
        }
        return interpretations.get(status, 'Unknown')
    
    def _get_fluency_interpretation(self, status):
        interpretations = {
            'Normal': 'Speech fluency and coherence within normal range',
            'Concerning': 'Mild fluency disruptions and occasional word-finding difficulties',
            'Impaired': 'Notable speech disfluencies and coherence issues',
            'Severely Impaired': 'Significant fluency breakdown and coherence deficits'
        }
        return interpretations.get(status, 'Unknown')
    
    def _get_prosodic_interpretation(self, status):
        interpretations = {
            'Normal': 'Prosodic features and intonation patterns appropriate',
            'Concerning': 'Mild reduction in prosodic variability',
            'Reduced Variability': 'Notable flattening of prosodic contours',
            'Severely Reduced Variability': 'Marked prosodic monotony suggesting neurological changes'
        }
        return interpretations.get(status, 'Unknown')
    
    def _generate_clinical_observations(self, features, risk_score):
        """Generate detailed clinical observations for medical professionals"""
        observations = {
            'pause_analysis': {
                'total_pause_time': features.get('pause_duration_mean', 0) * features.get('pause_rate', 0) * features.get('speech_time', 0),
                'pause_frequency': features.get('pause_rate', 0),
                'average_pause_duration': features.get('pause_duration_mean', 0),
                'pause_variability': features.get('pause_duration_std', 0),
                'clinical_significance': 'High' if features.get('pause_rate', 0) > 0.5 else 'Low'
            },
            'hesitation_patterns': {
                'filled_pauses': features.get('hesitation_count', 0),
                'word_repetitions': features.get('repetition_count', 0),
                'word_finding_episodes': max(features.get('hesitation_count', 0) - 2, 0),
                'severity': 'Severe' if features.get('hesitation_count', 0) > 10 else 'Mild' if features.get('hesitation_count', 0) > 5 else 'Normal'
            },
            'speech_motor_control': {
                'voice_tremor_indicator': features.get('jitter', 0),
                'amplitude_instability': features.get('shimmer', 0),
                'phonatory_control': features.get('voice_breaks', 0),
                'motor_speech_integrity': 'Compromised' if features.get('voice_breaks', 0) > 3 else 'Intact'
            },
            'cognitive_linguistic_markers': {
                'lexical_access_efficiency': features.get('type_token_ratio', 0),
                'semantic_network_integrity': features.get('semantic_fluency', 0),
                'discourse_coherence': features.get('coherence_score', 0),
                'information_content': features.get('idea_density', 0),
                'cognitive_load_indicators': features.get('hesitation_count', 0) + features.get('repetition_count', 0)
            },
            'overall_clinical_impression': self._generate_clinical_impression(features, risk_score)
        }
        return observations
    
    def _generate_clinical_impression(self, features, risk_score):
        """Generate overall clinical impression for medical professionals"""
        if risk_score >= 70:
            return {
                'severity': 'Severe',
                'impression': 'Multiple speech and language markers consistent with significant cognitive decline. Comprehensive neurological evaluation recommended.',
                'key_findings': ['Marked speech timing disruption', 'Significant lexical-semantic deficits', 'Notable fluency breakdown'],
                'follow_up': 'Urgent clinical assessment and cognitive testing advised'
            }
        elif risk_score >= 50:
            return {
                'severity': 'Moderate',
                'impression': 'Several concerning speech patterns that may indicate early cognitive changes. Clinical correlation recommended.',
                'key_findings': ['Speech timing alterations', 'Mild lexical difficulties', 'Fluency concerns'],
                'follow_up': 'Clinical assessment and monitoring recommended'
            }
        elif risk_score >= 30:
            return {
                'severity': 'Mild',
                'impression': 'Some speech pattern variations noted. May warrant monitoring and reassessment.',
                'key_findings': ['Subtle timing changes', 'Occasional word-finding difficulties'],
                'follow_up': 'Consider periodic reassessment'
            }
        else:
            return {
                'severity': 'Normal',
                'impression': 'Speech and language patterns within expected normal range.',
                'key_findings': ['Normal speech timing', 'Adequate lexical access', 'Good fluency'],
                'follow_up': 'No immediate concerns'
            }
    
    def transcribe_audio(self, audio_path):
        """Transcribe audio to text"""
        try:
            print("Starting transcription...")
            r = sr.Recognizer()
            
            # Enhanced recognizer settings for noisy environments
            r.energy_threshold = 300  # Lower threshold for quiet speech
            r.dynamic_energy_threshold = True
            r.dynamic_energy_adjustment_damping = 0.15
            r.dynamic_energy_ratio = 1.5
            r.pause_threshold = 0.8  # Longer pause threshold for processing
            r.operation_timeout = None
            r.phrase_threshold = 0.3
            r.non_speaking_duration = 0.8
            
            # Convert to WAV if needed
            try:
                audio = AudioSegment.from_file(audio_path)
                wav_path = audio_path.replace(os.path.splitext(audio_path)[1], '.wav')
                audio.export(wav_path, format="wav")
                print(f"Converted audio to WAV: {wav_path}")
            except Exception as conv_error:
                print(f"Audio conversion failed: {conv_error}")
                # Try to use original file if conversion fails
                wav_path = audio_path
            
            try:
                with sr.AudioFile(wav_path) as source:
                    # Enhanced ambient noise adjustment for noisy environments
                    r.adjust_for_ambient_noise(source, duration=1.0)  # Longer adjustment period
                    audio_data = r.record(source)
                    
                    # Try multiple recognition attempts with different settings
                    transcript = None
                    
                    # First attempt: Standard recognition
                    try:
                        transcript = r.recognize_google(audio_data, language='en-US')
                        print(f"Transcription successful (standard): {transcript[:50]}...")
                    except:
                        pass
                    
                    # Second attempt: With show_all for better noise handling
                    if not transcript:
                        try:
                            result = r.recognize_google(audio_data, language='en-US', show_all=True)
                            if result and 'alternative' in result and result['alternative']:
                                transcript = result['alternative'][0]['transcript']
                                print(f"Transcription successful (enhanced): {transcript[:50]}...")
                        except:
                            pass
                    
                    if transcript:
                        return transcript
                    else:
                        print("All transcription attempts failed")
                        return "[Speech not clearly audible]"
                        
            except sr.UnknownValueError:
                print("Speech recognition could not understand audio")
                return "[Speech not clearly audible]"
            except sr.RequestError as e:
                print(f"Could not request results from speech recognition service: {e}")
                return "[Transcription service unavailable]"
            
        except Exception as e:
            print(f"Transcription error: {e}")
            return "[Transcription failed]"
        finally:
            # Clean up temp file
            if 'wav_path' in locals() and wav_path != audio_path and os.path.exists(wav_path):
                try:
                    os.remove(wav_path)
                    print(f"Cleaned up WAV file: {wav_path}")
                except:
                    pass
    
    def analyze_audio(self, audio_path, task_type=None):
        """Complete audio analysis pipeline with task-specific analysis"""
        # Extract features
        acoustic_features = self.extract_acoustic_features(audio_path)
        
        # Transcribe and extract linguistic features
        transcript = self.transcribe_audio(audio_path)
        linguistic_features = self.extract_linguistic_features(transcript)
        
        # Task-specific analysis
        task_features = {}
        if task_type and task_type in self.speech_tasks:
            task_features = self._analyze_task_performance(transcript, task_type)
        
        # Combine all features
        all_features = {**acoustic_features, **linguistic_features, **task_features}
        
        # Enhanced feature extraction for better clinical relevance
        enhanced_features = self._extract_enhanced_features(acoustic_features, linguistic_features, transcript)
        all_features.update(enhanced_features)
        
        # Create feature vector - handle missing features gracefully
        feature_vector = []
        for name in self.feature_names:
            if name in all_features:
                feature_vector.append(all_features[name])
            else:
                # Provide reasonable defaults for missing features
                if 'rate' in name or 'time' in name:
                    feature_vector.append(120.0)  # Default speaking rate
                elif 'ratio' in name:
                    feature_vector.append(0.5)    # Default ratio
                elif 'count' in name:
                    feature_vector.append(0.0)    # Default count
                elif 'jitter' in name or 'shimmer' in name:
                    feature_vector.append(0.01)   # Default voice quality
                else:
                    feature_vector.append(0.0)    # General default
        
        feature_vector = np.array(feature_vector).reshape(1, -1)
        
        # Scale features
        feature_vector_scaled = self.scaler.transform(feature_vector)
        
        # Get prediction and probability
        prediction = self.model.predict(feature_vector_scaled)[0]
        probability = self.model.predict_proba(feature_vector_scaled)[0]
        
        # Calculate moderately harsh risk score
        raw_risk_score = probability[1] * 100  # Probability of AD
        
        # Moderately harsh calibration - between lenient and harsh
        calibrated_risk_score = max(raw_risk_score * 0.75, 8)  # Reduce by 25%, minimum 8
        
        risk_assessment = self._calculate_risk_assessment(calibrated_risk_score, all_features, task_type)
        
        # Generate comprehensive clinical observations
        clinical_observations = self._generate_clinical_observations(all_features, risk_assessment['score'])
        
        results = {
            'transcript': transcript,
            'features': all_features,
            'risk_score': risk_assessment['score'],
            'risk_assessment': risk_assessment,
            'biomarkers': self._generate_biomarker_report(all_features, risk_assessment['score']),
            'clinical_observations': clinical_observations
        }
        
        # Add task-specific results if applicable
        if task_type:
            results['task_analysis'] = {
                'task_type': task_type,
                'task_name': task_type.replace('_', ' ').title(),
                'performance_metrics': task_features.get('task_performance', {}),
                'task_specific_observations': self._generate_task_observations(task_type, task_features)
            }
        
        return results
    
    def _calculate_risk_assessment(self, base_score, features, task_type=None):
        """Calculate moderately harsh risk assessment with task-specific adjustments"""
        # Balanced baseline - normal speech should score low, but maintain sensitivity for AD
        score = max(base_score * 0.3, 5)  # 30% of base score, minimum 5 (lower baseline)
        
        risk_indicators = 0
        task_performance_penalty = 0
        
        # Task-specific performance adjustments
        if task_type:
            task_performance = features.get('task_performance', {})
            if task_type == 'picture_description':
                element_coverage = task_performance.get('element_coverage', 1.0)
                if element_coverage < 0.3:  # Very poor coverage
                    task_performance_penalty += 20
                    risk_indicators += 2
                elif element_coverage < 0.5:  # Poor coverage
                    task_performance_penalty += 10
                    risk_indicators += 1
            
            elif task_type == 'semantic_fluency':
                animal_count = task_performance.get('animal_count', 10)
                if animal_count < 5:  # Very few animals named
                    task_performance_penalty += 25
                    risk_indicators += 2
                elif animal_count < 8:  # Below average
                    task_performance_penalty += 12
                    risk_indicators += 1
            
            elif task_type == 'story_retelling':
                recall_accuracy = task_performance.get('recall_accuracy', 0.8)
                if recall_accuracy < 0.3:  # Very poor recall
                    task_performance_penalty += 22
                    risk_indicators += 2
                elif recall_accuracy < 0.5:  # Poor recall
                    task_performance_penalty += 12
                    risk_indicators += 1
        
        # Enhanced acoustic and linguistic feature analysis
        # SPEECH TIMING gets slightly higher weight
        
        # Speaking rate analysis (moderately strict thresholds) - INCREASED WEIGHT
        speaking_rate = features.get('speaking_rate', 120)
        if speaking_rate < 70:  # Very slow speech
            score += 26  # Increased from 22
            risk_indicators += 2
        elif speaking_rate < 90:  # Moderately slow
            score += 15  # Increased from 12
            risk_indicators += 1
        elif speaking_rate > 220:  # Unusually fast
            score += 10  # Increased from 8
            risk_indicators += 1
        
        # Enhanced pause analysis - INCREASED WEIGHT
        pause_rate = features.get('pause_rate', 0.2)
        pause_duration_mean = features.get('pause_duration_mean', 0.5)
        if pause_rate > 0.55:  # Excessive pauses
            score += 22  # Increased from 18
            risk_indicators += 2
        elif pause_rate > 0.35:  # Many pauses
            score += 13  # Increased from 10
            risk_indicators += 1
        
        # Long pause penalty - INCREASED WEIGHT
        if pause_duration_mean > 2.0:  # Very long pauses
            score += 15  # Increased from 12
            risk_indicators += 1
        
        # Speech rate variability (timing consistency) - NEW TIMING FEATURE
        speech_rate_variability = features.get('speech_rate_variability', 1.0)
        if speech_rate_variability > 3.0:  # Very inconsistent timing
            score += 8
            risk_indicators += 1
        
        # Enhanced lexical diversity analysis
        ttr = features.get('type_token_ratio', 0.5)
        if ttr < 0.28:  # Very low diversity
            score += 18
            risk_indicators += 2
        elif ttr < 0.38:  # Low diversity
            score += 10
            risk_indicators += 1
        
        # Voice quality analysis - REMOVED as environmental factor
        # Environmental factors like microphone quality, room acoustics, etc.
        # can significantly affect voice quality metrics, so we exclude them
        # from risk assessment to focus on actual speech patterns
        
        # Enhanced disfluency analysis
        hesitation_count = features.get('hesitation_count', 0)
        repetition_count = features.get('repetition_count', 0)
        
        if hesitation_count > 8:  # Many hesitations
            score += 15
            risk_indicators += 2
        elif hesitation_count > 4:
            score += 8
            risk_indicators += 1
        
        if repetition_count > 6:  # Many repetitions
            score += 12
            risk_indicators += 1
        elif repetition_count > 3:
            score += 6
        
        # Semantic and syntactic complexity
        semantic_fluency = features.get('semantic_fluency', 0.6)
        syntactic_complexity = features.get('syntactic_complexity', 8)
        
        if semantic_fluency < 0.4:  # Poor semantic fluency
            score += 12
            risk_indicators += 1
        
        if syntactic_complexity < 5:  # Very simple sentences
            score += 10
            risk_indicators += 1
        
        # Apply task performance penalty
        score += task_performance_penalty
        
        # Apply timing-weighted multiplier based on risk indicators
        # Give extra weight if timing issues are present
        timing_issues = 0
        if speaking_rate < 90 or speaking_rate > 200:
            timing_issues += 1
        if pause_rate > 0.35:
            timing_issues += 1
        if pause_duration_mean > 1.5:
            timing_issues += 1
        
        # Enhanced multiplier with timing bias
        if risk_indicators >= 4:
            multiplier = 1.4 + (timing_issues * 0.05)  # Extra boost for timing issues
            score *= multiplier
        elif risk_indicators >= 3:
            multiplier = 1.25 + (timing_issues * 0.04)
            score *= multiplier
        elif risk_indicators >= 2:
            multiplier = 1.15 + (timing_issues * 0.03)
            score *= multiplier
        
        final_score = min(score, 100)
        
        # Moderately harsh risk tiers - stricter than lenient, more reasonable than harsh
        if final_score >= 70 and risk_indicators >= 3:
            return {
                'tier': 'Very High',
                'score': final_score,
                'description': 'Multiple significant AD indicators present',
                'recommendation': 'Immediate comprehensive clinical evaluation recommended',
                'indicators_count': risk_indicators,
                'task_performance_impact': task_performance_penalty > 0
            }
        elif final_score >= 50 and risk_indicators >= 2:
            return {
                'tier': 'High',
                'score': final_score,
                'description': 'Several concerning AD markers detected',
                'recommendation': 'Clinical assessment strongly advised within 2-4 weeks',
                'indicators_count': risk_indicators,
                'task_performance_impact': task_performance_penalty > 0
            }
        elif final_score >= 32:
            return {
                'tier': 'Moderate',
                'score': final_score,
                'description': 'Some concerning speech and language patterns observed',
                'recommendation': 'Consider clinical consultation and follow-up assessment',
                'indicators_count': risk_indicators,
                'task_performance_impact': task_performance_penalty > 0
            }
        elif final_score >= 15:
            return {
                'tier': 'Low-Moderate',
                'score': final_score,
                'description': 'Mild indicators detected, may warrant monitoring',
                'recommendation': 'Monitor speech patterns and reassess in 3-6 months',
                'indicators_count': risk_indicators
            }
        else:
            return {
                'tier': 'Normal',
                'score': final_score,
                'description': 'Speech patterns within normal range',
                'recommendation': 'No concerns detected',
                'indicators_count': risk_indicators
            }
    
    def _extract_enhanced_features(self, acoustic_features, linguistic_features, transcript):
        """Extract additional advanced features for better clinical assessment"""
        enhanced = {}
        
        words = transcript.lower().split()
        if not words:
            return enhanced
        
        # Advanced temporal features
        enhanced['speech_rate_variability'] = acoustic_features.get('pause_duration_std', 0) / max(acoustic_features.get('pause_duration_mean', 1), 0.1)
        enhanced['articulation_rate'] = acoustic_features.get('speaking_rate', 120) * (1 - acoustic_features.get('pause_rate', 0.2))
        
        # Noise robustness features
        enhanced['signal_quality'] = self._estimate_signal_quality(acoustic_features)
        enhanced['noise_robustness_score'] = min(enhanced['signal_quality'] * 1.2, 1.0)
        
        # Advanced lexical features
        enhanced['word_length_variability'] = np.std([len(word) for word in words]) if len(words) > 1 else 0
        enhanced['function_word_ratio'] = self._calculate_function_word_ratio(words)
        enhanced['content_word_density'] = 1 - enhanced['function_word_ratio']
        
        # Discourse-level features
        enhanced['idea_density'] = self._calculate_idea_density(transcript)
        enhanced['propositional_density'] = self._calculate_propositional_density(words)
        enhanced['narrative_coherence'] = self._calculate_narrative_coherence(transcript)
        
        # Phonetic complexity features
        enhanced['phonetic_complexity'] = self._estimate_phonetic_complexity(words)
        enhanced['syllable_complexity'] = self._estimate_syllable_complexity(words)
        
        # Cognitive load indicators
        enhanced['cognitive_load_index'] = (
            acoustic_features.get('hesitation_count', 0) * 0.3 +
            acoustic_features.get('repetition_count', 0) * 0.2 +
            acoustic_features.get('pause_rate', 0) * 10 +
            (1 - linguistic_features.get('type_token_ratio', 0.5)) * 5
        )
        
        return enhanced
    
    def _calculate_function_word_ratio(self, words):
        """Calculate ratio of function words to total words"""
        function_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall', 'this', 'that',
            'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'
        }
        function_count = sum(1 for word in words if word.lower() in function_words)
        return function_count / len(words) if words else 0
    
    def _calculate_idea_density(self, transcript):
        """Estimate idea density in the transcript"""
        sentences = [s.strip() for s in transcript.split('.') if s.strip()]
        if not sentences:
            return 0
        
        total_ideas = 0
        for sentence in sentences:
            words = sentence.split()
            content_words = [w for w in words if len(w) > 3 and w.lower() not in {'this', 'that', 'with', 'from', 'they', 'them', 'were', 'been', 'have'}]
            total_ideas += len(content_words)
        
        total_words = len(transcript.split())
        return total_ideas / total_words if total_words > 0 else 0
    
    def _calculate_propositional_density(self, words):
        """Estimate propositional density"""
        if len(words) < 5:
            return 0
        
        verb_indicators = ['ed', 'ing', 'er', 'est']
        proposition_count = 0
        
        for word in words:
            if any(word.endswith(suffix) for suffix in verb_indicators):
                proposition_count += 1
            elif len(word) > 6:
                proposition_count += 0.5
        
        return proposition_count / len(words)
    
    def _calculate_narrative_coherence(self, transcript):
        """Estimate narrative coherence"""
        sentences = [s.strip() for s in transcript.split('.') if s.strip()]
        if len(sentences) < 2:
            return 1.0
        
        coherence_scores = []
        for i in range(len(sentences) - 1):
            words1 = set(sentences[i].lower().split())
            words2 = set(sentences[i + 1].lower().split())
            if words1 and words2:
                overlap = len(words1.intersection(words2))
                coherence = overlap / min(len(words1), len(words2))
                coherence_scores.append(coherence)
        
        return np.mean(coherence_scores) if coherence_scores else 0
    
    def _estimate_phonetic_complexity(self, words):
        """Estimate phonetic complexity of speech"""
        if not words:
            return 0
        
        complex_sounds = ['th', 'ch', 'sh', 'ph', 'gh', 'ck', 'ng']
        complexity_score = 0
        
        for word in words:
            word_lower = word.lower()
            for sound in complex_sounds:
                complexity_score += word_lower.count(sound)
        
        return complexity_score / len(words)
    
    def _estimate_syllable_complexity(self, words):
        """Estimate average syllable complexity"""
        if not words:
            return 0
        
        total_syllables = 0
        for word in words:
            vowels = 'aeiouy'
            syllables = sum(1 for char in word.lower() if char in vowels)
            syllables = max(syllables, 1)
            total_syllables += syllables
        
        return total_syllables / len(words)
    
    def _estimate_signal_quality(self, acoustic_features):
        """Estimate signal quality for noise robustness assessment"""
        # Simple heuristic based on voice quality metrics
        jitter = acoustic_features.get('jitter', 0.01)
        shimmer = acoustic_features.get('shimmer', 0.05)
        voice_breaks = acoustic_features.get('voice_breaks', 1)
        
        # Good signal quality has low jitter, low shimmer, few voice breaks
        quality_score = 1.0
        
        if jitter > 0.02:
            quality_score -= 0.2
        if shimmer > 0.08:
            quality_score -= 0.2
        if voice_breaks > 3:
            quality_score -= 0.3
        
        return max(quality_score, 0.1)
    
    def _generate_task_observations(self, task_type, task_features):
        """Generate task-specific clinical observations"""
        task_performance = task_features.get('task_performance', {})
        
        if task_type == 'picture_description':
            element_coverage = task_performance.get('element_coverage', 1.0)
            return {
                'task_completion': 'Complete' if element_coverage > 0.7 else 'Partial' if element_coverage > 0.4 else 'Incomplete',
                'detail_level': 'High' if element_coverage > 0.8 else 'Moderate' if element_coverage > 0.5 else 'Low',
                'clinical_significance': 'Poor visual-semantic integration' if element_coverage < 0.3 else 'Normal visual processing'
            }
        
        elif task_type == 'semantic_fluency':
            animal_count = task_performance.get('animal_count', 10)
            clustering = task_performance.get('semantic_clustering', 0.5)
            return {
                'fluency_score': 'Above Average' if animal_count > 15 else 'Average' if animal_count > 8 else 'Below Average',
                'semantic_access': 'Efficient' if clustering > 0.6 else 'Reduced efficiency',
                'clinical_significance': 'Semantic network disruption' if animal_count < 5 else 'Normal semantic access'
            }
        
        elif task_type == 'story_retelling':
            recall_accuracy = task_performance.get('recall_accuracy', 0.8)
            return {
                'memory_performance': 'Good' if recall_accuracy > 0.7 else 'Fair' if recall_accuracy > 0.4 else 'Poor',
                'narrative_structure': 'Preserved' if recall_accuracy > 0.6 else 'Compromised',
                'clinical_significance': 'Memory encoding/retrieval deficit' if recall_accuracy < 0.3 else 'Normal memory function'
            }
        
        elif task_type == 'procedural_description':
            step_coverage = task_performance.get('step_coverage', 0.8)
            return {
                'procedural_knowledge': 'Intact' if step_coverage > 0.7 else 'Impaired',
                'sequential_processing': 'Normal' if step_coverage > 0.6 else 'Disrupted',
                'clinical_significance': 'Executive function deficit' if step_coverage < 0.4 else 'Normal procedural memory'
            }
        
        elif task_type == 'spontaneous_speech':
            coherence = task_performance.get('coherence_score', 0.7)
            return {
                'discourse_coherence': 'High' if coherence > 0.7 else 'Moderate' if coherence > 0.5 else 'Low',
                'spontaneous_organization': 'Well-organized' if coherence > 0.6 else 'Disorganized',
                'clinical_significance': 'Discourse planning deficit' if coherence < 0.4 else 'Normal discourse ability'
            }
        
        return {'task_completion': 'Unknown', 'clinical_significance': 'Unable to assess'}
    
    def _enhance_audio_quality(self, y, sr):
        """Apply noise reduction and audio enhancement for better analysis"""
        try:
            # 1. Normalize audio amplitude
            y = y / (np.max(np.abs(y)) + 1e-8)
            
            # 2. Apply high-pass filter to remove low-frequency noise (AC hum, etc.)
            nyquist = sr / 2
            high_cutoff = 80 / nyquist  # Remove frequencies below 80Hz
            b, a = butter(4, high_cutoff, btype='high')
            y = filtfilt(b, a, y)
            
            # 3. Apply low-pass filter to remove high-frequency noise
            low_cutoff = 8000 / nyquist  # Keep frequencies below 8kHz (speech range)
            b, a = butter(4, low_cutoff, btype='low')
            y = filtfilt(b, a, y)
            
            # 4. Spectral subtraction for background noise reduction
            y = self._spectral_subtraction(y, sr)
            
            # 5. Dynamic range compression to enhance quiet speech
            y = self._dynamic_range_compression(y)
            
            # 6. Final normalization
            y = y / (np.max(np.abs(y)) + 1e-8) * 0.9
            
            print("Audio enhancement applied: noise reduction, filtering, compression")
            return y
            
        except Exception as e:
            print(f"Audio enhancement failed: {e}, using original audio")
            return y
    
    def _spectral_subtraction(self, y, sr, alpha=2.0, beta=0.01):
        """Apply spectral subtraction to reduce background noise"""
        try:
            # STFT parameters
            n_fft = 2048
            hop_length = 512
            
            # Compute STFT
            stft = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # Estimate noise from first 0.5 seconds (assumed to be mostly noise)
            noise_frames = int(0.5 * sr / hop_length)
            noise_spectrum = np.mean(magnitude[:, :noise_frames], axis=1, keepdims=True)
            
            # Apply spectral subtraction
            enhanced_magnitude = magnitude - alpha * noise_spectrum
            
            # Ensure we don't over-subtract (keep at least beta of original)
            enhanced_magnitude = np.maximum(enhanced_magnitude, beta * magnitude)
            
            # Reconstruct signal
            enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
            enhanced_audio = librosa.istft(enhanced_stft, hop_length=hop_length)
            
            return enhanced_audio
            
        except Exception as e:
            print(f"Spectral subtraction failed: {e}")
            return y
    
    def _dynamic_range_compression(self, y, threshold=0.1, ratio=4.0):
        """Apply dynamic range compression to enhance quiet speech"""
        try:
            # Convert to dB scale
            y_abs = np.abs(y)
            y_db = 20 * np.log10(y_abs + 1e-8)
            
            # Apply compression above threshold
            threshold_db = 20 * np.log10(threshold)
            compressed_db = np.where(
                y_db > threshold_db,
                threshold_db + (y_db - threshold_db) / ratio,
                y_db
            )
            
            # Convert back to linear scale
            compressed_magnitude = 10 ** (compressed_db / 20)
            
            # Preserve original sign
            y_compressed = compressed_magnitude * np.sign(y)
            
            return y_compressed
            
        except Exception as e:
            print(f"Dynamic range compression failed: {e}")
            return y
    
    def _analyze_task_performance(self, transcript, task_type):
        """Analyze performance on specific speech tasks"""
        words = transcript.lower().split()
        task_performance = {}
        
        if task_type == 'picture_description':
            # Expected elements for picture description
            expected_elements = ['person', 'people', 'man', 'woman', 'child', 'house', 'tree', 'car', 'dog', 'cat', 'sitting', 'standing', 'walking', 'running', 'playing']
            found_elements = sum(1 for element in expected_elements if element in transcript.lower())
            element_coverage = found_elements / len(expected_elements)
            
            task_performance = {
                'element_coverage': element_coverage,
                'elements_found': found_elements,
                'total_expected': len(expected_elements),
                'detail_level': len(words) / 30 if len(words) > 0 else 0  # Normalize by expected length
            }
        
        elif task_type == 'semantic_fluency':
            # Multi-category semantic fluency analysis
            categories = {
                'animals': ['dog', 'cat', 'bird', 'fish', 'horse', 'cow', 'pig', 'sheep', 'goat', 'chicken', 'duck', 'rabbit', 'mouse', 'rat', 'elephant', 'lion', 'tiger', 'bear', 'wolf', 'fox', 'deer', 'monkey', 'zebra', 'giraffe', 'hippo'],
                'fruits': ['apple', 'banana', 'orange', 'grape', 'strawberry', 'peach', 'pear', 'cherry', 'plum', 'mango', 'pineapple', 'watermelon', 'lemon', 'lime', 'kiwi'],
                'vehicles': ['car', 'truck', 'bus', 'train', 'plane', 'boat', 'ship', 'bicycle', 'motorcycle', 'helicopter', 'taxi', 'van', 'subway', 'scooter', 'ambulance'],
                'clothing': ['shirt', 'pants', 'dress', 'shoes', 'socks', 'hat', 'jacket', 'coat', 'sweater', 'skirt', 'tie', 'belt', 'gloves', 'scarf', 'underwear'],
                'kitchen': ['knife', 'fork', 'spoon', 'plate', 'bowl', 'cup', 'pan', 'pot', 'stove', 'oven', 'refrigerator', 'microwave', 'blender', 'toaster', 'sink'],
                'colors': ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'black', 'white', 'brown', 'gray', 'silver', 'gold', 'violet', 'turquoise'],
                'sports': ['football', 'basketball', 'baseball', 'soccer', 'tennis', 'golf', 'swimming', 'running', 'boxing', 'hockey', 'volleyball', 'cricket', 'rugby', 'skiing', 'cycling'],
                'body_parts': ['head', 'arm', 'leg', 'hand', 'foot', 'eye', 'ear', 'nose', 'mouth', 'finger', 'toe', 'knee', 'elbow', 'shoulder', 'back'],
                'professions': ['doctor', 'teacher', 'nurse', 'lawyer', 'engineer', 'chef', 'pilot', 'police', 'firefighter', 'artist', 'writer', 'musician', 'scientist', 'farmer', 'mechanic'],
                'instruments': ['piano', 'guitar', 'violin', 'drums', 'flute', 'trumpet', 'saxophone', 'clarinet', 'harp', 'cello', 'tuba', 'harmonica', 'organ', 'banjo', 'xylophone']
            }
            
            # Determine which category based on most matches
            category_counts = {}
            for category, items in categories.items():
                count = sum(1 for item in items if item in transcript.lower())
                if count > 0:
                    category_counts[category] = count
            
            if category_counts:
                primary_category = max(category_counts, key=category_counts.get)
                item_count = category_counts[primary_category]
                
                # Calculate clustering for the primary category
                if primary_category == 'animals':
                    farm_animals = sum(1 for animal in ['cow', 'pig', 'sheep', 'goat', 'chicken', 'duck'] if animal in transcript.lower())
                    wild_animals = sum(1 for animal in ['lion', 'tiger', 'bear', 'wolf', 'elephant', 'zebra'] if animal in transcript.lower())
                    pets = sum(1 for animal in ['dog', 'cat', 'bird', 'fish', 'rabbit'] if animal in transcript.lower())
                    clusters = sum(1 for count in [farm_animals, wild_animals, pets] if count > 0)
                    semantic_clustering = min(clusters / 3.0, 1.0)
                else:
                    semantic_clustering = 0.7  # Default for other categories
            else:
                item_count = 0
                semantic_clustering = 0.0
                primary_category = 'unknown'
            
            task_performance = {
                'item_count': item_count,
                'category': primary_category,
                'semantic_clustering': semantic_clustering,
                'fluency_rate': item_count / max(len(words) / 60, 1)
            }
        
        elif task_type == 'story_retelling':
            # Dynamic story element detection based on common story themes
            story_keywords = {
                'characters': ['woman', 'man', 'girl', 'boy', 'mother', 'father', 'child', 'friend', 'baker', 'student', 'librarian', 'family', 'dog', 'stranger'],
                'objects': ['cookie', 'jar', 'stool', 'car', 'store', 'sugar', 'rain', 'cafe', 'toy', 'playground', 'bird', 'sandwich', 'picnic', 'window', 'book'],
                'actions': ['baking', 'climbing', 'fell', 'broke', 'walking', 'running', 'looking', 'searching', 'feeding', 'caring', 'sharing', 'planning', 'cleaning', 'investigating'],
                'locations': ['kitchen', 'store', 'park', 'home', 'garden', 'school', 'shop', 'library', 'yard', 'theater']
            }
            
            total_elements = 0
            found_elements = 0
            for category, elements in story_keywords.items():
                category_found = sum(1 for element in elements if element in transcript.lower())
                found_elements += category_found
                total_elements += len(elements)
            
            recall_accuracy = found_elements / total_elements if total_elements > 0 else 0
            
            task_performance = {
                'recall_accuracy': recall_accuracy,
                'elements_recalled': found_elements,
                'total_elements': total_elements,
                'narrative_length': len(words)
            }
        
        elif task_type == 'procedural_description':
            # Multi-task procedural analysis
            procedural_keywords = {
                'sandwich': ['bread', 'slice', 'spread', 'butter', 'jam', 'peanut', 'meat', 'cheese', 'lettuce', 'tomato', 'put', 'together', 'cut'],
                'teeth': ['brush', 'toothbrush', 'toothpaste', 'water', 'rinse', 'spit', 'clean', 'mouth', 'gums', 'circular'],
                'coffee': ['water', 'coffee', 'filter', 'pot', 'cup', 'pour', 'heat', 'boil', 'grind', 'beans', 'sugar', 'cream'],
                'shoelaces': ['lace', 'shoe', 'loop', 'tie', 'knot', 'pull', 'thread', 'hole', 'tight', 'bow'],
                'dishes': ['soap', 'water', 'sponge', 'scrub', 'rinse', 'dry', 'towel', 'clean', 'plate', 'bowl'],
                'eggs': ['egg', 'pan', 'heat', 'oil', 'butter', 'crack', 'scramble', 'stir', 'cook', 'salt', 'pepper'],
                'gift': ['paper', 'wrap', 'tape', 'scissors', 'fold', 'cut', 'bow', 'ribbon', 'box', 'cover'],
                'plant': ['seed', 'soil', 'pot', 'water', 'plant', 'dig', 'cover', 'sun', 'grow', 'care'],
                'lightbulb': ['bulb', 'light', 'switch', 'screw', 'socket', 'turn', 'replace', 'safe', 'power'],
                'airplane': ['paper', 'fold', 'crease', 'wing', 'point', 'throw', 'fly', 'straight', 'sharp']
            }
            
            # Determine which procedure based on most matches
            procedure_counts = {}
            for procedure, steps in procedural_keywords.items():
                count = sum(1 for step in steps if step in transcript.lower())
                if count > 0:
                    procedure_counts[procedure] = count
            
            if procedure_counts:
                primary_procedure = max(procedure_counts, key=procedure_counts.get)
                steps_mentioned = procedure_counts[primary_procedure]
                total_steps = len(procedural_keywords[primary_procedure])
                step_coverage = steps_mentioned / total_steps
            else:
                primary_procedure = 'unknown'
                steps_mentioned = 0
                total_steps = 10  # Default
                step_coverage = 0
            
            task_performance = {
                'step_coverage': step_coverage,
                'steps_mentioned': steps_mentioned,
                'total_steps': total_steps,
                'procedure_type': primary_procedure,
                'procedural_detail': len(words) / 40
            }
        
        elif task_type == 'spontaneous_speech':
            # Analyze coherence and organization
            sentences = [s.strip() for s in transcript.split('.') if s.strip()]
            coherence_score = self._calculate_narrative_coherence(transcript)
            
            task_performance = {
                'coherence_score': coherence_score,
                'sentence_count': len(sentences),
                'average_sentence_length': np.mean([len(s.split()) for s in sentences]) if sentences else 0,
                'topic_maintenance': min(coherence_score * 1.2, 1.0)  # Boost coherence for topic maintenance
            }
        
        return {'task_performance': task_performance}
    
    def _generate_biomarker_report(self, features, risk_score):
        """Generate detailed biomarker breakdown with accurate status reflection"""
        
        # Speech timing analysis - Enhanced weighting for clinical significance
        speaking_rate = features.get('speaking_rate', 120)
        pause_rate = features.get('pause_rate', 0.2)
        articulation_rate = features.get('articulation_rate', 120)
        speech_rate_variability = features.get('speech_rate_variability', 1.0)
        
        # More sensitive timing thresholds due to increased clinical importance
        if speaking_rate < 85 or pause_rate > 0.6 or speech_rate_variability > 3.0:
            timing_status = 'Severely Impaired'
        elif speaking_rate < 95 or pause_rate > 0.45 or speech_rate_variability > 2.0:
            timing_status = 'Impaired'
        elif speaking_rate < 105 or pause_rate > 0.32 or speech_rate_variability > 1.5:
            timing_status = 'Concerning'
        else:
            timing_status = 'Normal'
        
        # Voice quality analysis - Excluded from risk assessment due to environmental factors
        # Environmental conditions (microphone, room acoustics, etc.) heavily influence these metrics
        voice_breaks = features.get('voice_breaks', 0)
        jitter = features.get('jitter', 0.01)
        shimmer = features.get('shimmer', 0.05)
        voice_status = 'Not Assessed (Environmental Factor)'
        
        # Lexical-semantic analysis
        ttr = features.get('type_token_ratio', 0.5)
        semantic_fluency = features.get('semantic_fluency', 0.5)
        idea_density = features.get('idea_density', 1.0)
        
        if ttr < 0.25 or semantic_fluency < 0.3:
            lexical_status = 'Severely Reduced'
        elif ttr < 0.35 or semantic_fluency < 0.4:
            lexical_status = 'Reduced'
        elif ttr < 0.45 or semantic_fluency < 0.5:
            lexical_status = 'Concerning'
        else:
            lexical_status = 'Normal'
        
        # Fluency and coherence analysis
        hesitation_count = features.get('hesitation_count', 0)
        repetition_count = features.get('repetition_count', 0)
        coherence_score = features.get('coherence_score', 0.7)
        
        if hesitation_count > 10 or repetition_count > 8 or coherence_score < 0.4:
            fluency_status = 'Severely Impaired'
        elif hesitation_count > 6 or repetition_count > 5 or coherence_score < 0.5:
            fluency_status = 'Impaired'
        elif hesitation_count > 3 or repetition_count > 3 or coherence_score < 0.6:
            fluency_status = 'Concerning'
        else:
            fluency_status = 'Normal'
        
        # Prosodic features analysis
        f0_std = features.get('f0_std', 30)
        f0_range = features.get('f0_range', 100)
        
        if f0_std < 15 or f0_range < 50:
            prosodic_status = 'Severely Reduced Variability'
        elif f0_std < 20 or f0_range < 75:
            prosodic_status = 'Reduced Variability'
        elif f0_std < 25:
            prosodic_status = 'Concerning'
        else:
            prosodic_status = 'Normal'
        
        # Generate clinical domain analysis with specific triggers and recommendations
        clinical_domains = self._analyze_clinical_domains(features)
        
        return {
            'speech_timing': {
                'speaking_rate': speaking_rate,
                'pause_rate': pause_rate,
                'articulation_rate': articulation_rate,
                'pause_duration_mean': features.get('pause_duration_mean', 0),
                'status': timing_status,
                'interpretation': self._get_timing_interpretation(timing_status)
            },
            'voice_quality': {
                'jitter': jitter,
                'shimmer': shimmer,
                'hnr': features.get('hnr', 10),
                'voice_breaks': voice_breaks,
                'status': voice_status,
                'interpretation': self._get_voice_interpretation(voice_status)
            },
            'lexical_semantic': {
                'type_token_ratio': ttr,
                'semantic_fluency': semantic_fluency,
                'idea_density': idea_density,
                'unique_words': features.get('unique_words', 0),
                'status': lexical_status,
                'interpretation': self._get_lexical_interpretation(lexical_status)
            },
            'fluency_coherence': {
                'hesitation_count': hesitation_count,
                'repetition_count': repetition_count,
                'coherence_score': coherence_score,
                'word_finding_difficulty': hesitation_count + repetition_count,
                'status': fluency_status,
                'interpretation': self._get_fluency_interpretation(fluency_status)
            },
            'prosodic_features': {
                'f0_mean': features.get('f0_mean', 150),
                'f0_std': f0_std,
                'f0_range': f0_range,
                'spectral_centroid': features.get('spectral_centroid', 2000),
                'status': prosodic_status,
                'interpretation': self._get_prosodic_interpretation(prosodic_status)
            },
            'clinical_observations': self._generate_clinical_observations(features, risk_score),
            'clinical_domains': clinical_domains
        }
    
    def _analyze_clinical_domains(self, features):
        """Analyze clinical domains with specific triggers and doctor recommendations"""
        domains = {}
        
        # 1. Fluency & Coherence Domain
        hesitation_count = features.get('hesitation_count', 0)
        repetition_count = features.get('repetition_count', 0)
        coherence_score = features.get('coherence_score', 0.7)
        
        fluency_triggered = hesitation_count > 6 or repetition_count > 5 or coherence_score < 0.5
        domains['fluency_coherence'] = {
            'triggered': fluency_triggered,
            'severity': self._get_severity_level(hesitation_count, repetition_count, coherence_score),
            'vulnerability': 'Word finding difficulty or disrupted narrative flow' if fluency_triggered else 'Normal narrative flow',
            'clinical_indicators': {
                'hesitation_count': hesitation_count,
                'repetition_count': repetition_count,
                'coherence_score': coherence_score
            },
            'doctor_recommendations': [
                'Administer verbal fluency tasks (e.g., "name animals in 60s")',
                'Conduct memory recall tests to differentiate lexical vs memory-driven pauses',
                'Consider Boston Naming Test for word-finding assessment',
                'Evaluate narrative discourse with picture description tasks'
            ] if fluency_triggered else ['Continue routine cognitive monitoring']
        }
        
        # 2. Lexical Semantic Domain
        idea_density = features.get('idea_density', 0.5)
        semantic_fluency = features.get('semantic_fluency', 0.5)
        type_token_ratio = features.get('type_token_ratio', 0.5)
        
        lexical_triggered = idea_density < 0.4 or semantic_fluency < 0.4 or type_token_ratio < 0.35
        domains['lexical_semantic'] = {
            'triggered': lexical_triggered,
            'severity': self._get_lexical_severity(idea_density, semantic_fluency, type_token_ratio),
            'vulnerability': 'Reduced vocabulary richness or impaired semantic access' if lexical_triggered else 'Normal semantic processing',
            'clinical_indicators': {
                'idea_density': idea_density,
                'semantic_fluency': semantic_fluency,
                'type_token_ratio': type_token_ratio
            },
            'doctor_recommendations': [
                'Use Cookie Theft picture description task for semantic evaluation',
                'Administer category generation tasks (animals, foods, etc.)',
                'Consider semantic verbal fluency tests',
                'Evaluate with Pyramids and Palm Trees test for semantic associations',
                'Assess reading comprehension and naming abilities'
            ] if lexical_triggered else ['Monitor vocabulary diversity in future sessions']
        }
        
        # 3. Prosodic Features Domain
        f0_range = features.get('f0_range', 100)
        f0_std = features.get('f0_std', 25)
        spectral_centroid = features.get('spectral_centroid', 2000)
        
        prosodic_triggered = f0_range < 75 or f0_std < 20
        domains['prosodic_features'] = {
            'triggered': prosodic_triggered,
            'severity': self._get_prosodic_severity(f0_range, f0_std),
            'vulnerability': 'Monotone speech ‚Üí possible emotional blunting, motor deficits, or apathy' if prosodic_triggered else 'Normal prosodic variation',
            'clinical_indicators': {
                'f0_range': f0_range,
                'f0_std': f0_std,
                'spectral_variation': spectral_centroid
            },
            'doctor_recommendations': [
                'Assess emotional prosody tasks (repeat sentences with different emotions)',
                'Check for motor speech disorders (dysarthria screening)',
                'Evaluate for depression or apathy using standardized scales',
                'Consider neurological consultation for motor speech assessment',
                'Rule out medication effects on speech prosody'
            ] if prosodic_triggered else ['Continue monitoring prosodic patterns']
        }
        
        # 4. Speech Timing Domain
        articulation_rate = features.get('articulation_rate', 120)
        pause_duration_mean = features.get('pause_duration_mean', 0.5)
        pause_rate = features.get('pause_rate', 0.25)
        speech_rate_variability = features.get('speech_rate_variability', 1.0)
        
        timing_triggered = articulation_rate < 95 or pause_duration_mean > 1.0 or pause_rate > 0.45
        domains['speech_timing'] = {
            'triggered': timing_triggered,
            'severity': self._get_timing_severity(articulation_rate, pause_duration_mean, pause_rate),
            'vulnerability': 'Disrupted speech planning or working memory overload' if timing_triggered else 'Normal speech timing',
            'clinical_indicators': {
                'articulation_rate': articulation_rate,
                'pause_duration_mean': pause_duration_mean,
                'pause_rate': pause_rate,
                'speech_rate_variability': speech_rate_variability
            },
            'doctor_recommendations': [
                'Run dual-task speech tests (counting while speaking)',
                'Administer timed verbal fluency under cognitive load',
                'Evaluate working memory with digit span tasks',
                'Consider speech-language pathology consultation',
                'Assess attention and executive function'
            ] if timing_triggered else ['Monitor speech timing consistency']
        }
        
        # 5. Voice Quality Domain
        jitter = features.get('jitter', 0.01)
        shimmer = features.get('shimmer', 0.05)
        hnr = features.get('hnr', 15)
        voice_breaks = features.get('voice_breaks', 0)
        
        voice_triggered = jitter > 0.03 or shimmer > 0.07 or hnr < 12 or voice_breaks > 3
        domains['voice_quality'] = {
            'triggered': voice_triggered,
            'severity': self._get_voice_severity(jitter, shimmer, hnr, voice_breaks),
            'vulnerability': 'Voice instability (may not be AD-specific, could be laryngeal or neurological)' if voice_triggered else 'Normal voice quality',
            'clinical_indicators': {
                'jitter': jitter,
                'shimmer': shimmer,
                'hnr': hnr,
                'voice_breaks': voice_breaks
            },
            'environmental_warning': 'IMPORTANT: Voice quality can be significantly affected by recording conditions (background noise, microphone type, device placement, room acoustics, distance from microphone). Consider re-recording in optimal conditions before clinical interpretation.' if voice_triggered else None,
            'doctor_recommendations': [
                'FIRST: Verify recording quality - check for background noise, poor microphone, or suboptimal recording conditions',
                'Re-record in quiet environment with good microphone if possible',
                'Refer for ENT evaluation to rule out structural laryngeal causes',
                'Cross-check with acoustic biomarkers in future sessions',
                'Consider voice therapy consultation if persistent',
                'Evaluate for neurological causes of voice instability',
                'Rule out medication effects on voice quality'
            ] if voice_triggered else ['Continue voice quality monitoring']
        }
        
        return domains
    
    def _get_severity_level(self, hesitation, repetition, coherence):
        """Determine severity for fluency/coherence domain"""
        if hesitation > 10 or repetition > 8 or coherence < 0.4:
            return 'Severe'
        elif hesitation > 6 or repetition > 5 or coherence < 0.5:
            return 'Moderate'
        elif hesitation > 3 or repetition > 3 or coherence < 0.6:
            return 'Mild'
        return 'Normal'
    
    def _get_lexical_severity(self, idea_density, semantic_fluency, ttr):
        """Determine severity for lexical-semantic domain"""
        if idea_density < 0.3 or semantic_fluency < 0.3 or ttr < 0.25:
            return 'Severe'
        elif idea_density < 0.4 or semantic_fluency < 0.4 or ttr < 0.35:
            return 'Moderate'
        elif idea_density < 0.5 or semantic_fluency < 0.5 or ttr < 0.45:
            return 'Mild'
        return 'Normal'
    
    def _get_prosodic_severity(self, f0_range, f0_std):
        """Determine severity for prosodic domain"""
        if f0_range < 50 or f0_std < 15:
            return 'Severe'
        elif f0_range < 75 or f0_std < 20:
            return 'Moderate'
        elif f0_range < 90 or f0_std < 25:
            return 'Mild'
        return 'Normal'
    
    def _get_timing_severity(self, articulation_rate, pause_duration, pause_rate):
        """Determine severity for speech timing domain"""
        if articulation_rate < 80 or pause_duration > 1.5 or pause_rate > 0.6:
            return 'Severe'
        elif articulation_rate < 95 or pause_duration > 1.0 or pause_rate > 0.45:
            return 'Moderate'
        elif articulation_rate < 110 or pause_duration > 0.8 or pause_rate > 0.35:
            return 'Mild'
        return 'Normal'
    
    def _get_voice_severity(self, jitter, shimmer, hnr, voice_breaks):
        """Determine severity for voice quality domain"""
        if jitter > 0.05 or shimmer > 0.1 or hnr < 10 or voice_breaks > 5:
            return 'Severe'
        elif jitter > 0.03 or shimmer > 0.07 or hnr < 12 or voice_breaks > 3:
            return 'Moderate'
        elif jitter > 0.02 or shimmer > 0.06 or hnr < 15 or voice_breaks > 2:
            return 'Mild'
        return 'Normal'

    def generate_realistic_transcript(self, risk_level):
        """Generate realistic transcript based on risk level"""
        if risk_level >= 80:
            return "I was... um... trying to... what do you call it... the thing where you... you know... when you want to... I can't remember the word... it's like... um..."
        elif risk_level >= 55:
            return "Well, I was going to the... uh... the place where they have... you know... the things... um... I think it was yesterday... or maybe... I'm not sure..."
        elif risk_level >= 30:
            return "I went to the store yesterday and, uh, I bought some things. There were... um... several items I needed but I can't recall all of them right now."
        elif risk_level >= 15:
            return "Yesterday I went shopping and bought groceries. I got most of what I needed, though I think I forgot a few items."
        else:
            return "Yesterday I went to the grocery store and bought everything on my list. I picked up fresh vegetables, milk, bread, and some fruit for the week."

# Initialize the analyzer
analyzer = AlzheimersVoiceAnalyzer()

@app.route('/tasks')
def get_tasks():
    """Get available speech tasks with random prompt selection"""
    import random
    tasks = {}
    for task_id, task_info in analyzer.speech_tasks.items():
        # Randomly select one prompt from the available prompts
        selected_prompt = random.choice(task_info['prompts'])
        tasks[task_id] = {
            'name': task_info['name'],
            'prompt': selected_prompt,
            'target_duration': task_info['target_duration'],
            'analysis_focus': task_info['analysis_focus']
        }
    return jsonify(tasks)

@app.route('/tasks/<task_id>')
def get_new_prompt(task_id):
    """Get a new random prompt for a specific task"""
    import random
    if task_id in analyzer.speech_tasks:
        task_info = analyzer.speech_tasks[task_id]
        selected_prompt = random.choice(task_info['prompts'])
        return jsonify({
            'name': task_info['name'],
            'prompt': selected_prompt,
            'target_duration': task_info['target_duration'],
            'analysis_focus': task_info['analysis_focus']
        })
    else:
        return jsonify({'error': 'Task not found'}), 404

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze_audio():
    tmp_file_path = None
    try:
        print("=== Starting audio analysis ===")
        
        if 'audio' not in request.files:
            print("Error: No audio file in request")
            return jsonify({'error': 'No audio file provided'}), 400
        
        audio_file = request.files['audio']
        if audio_file.filename == '':
            print("Error: Empty filename")
            return jsonify({'error': 'No file selected'}), 400
        
        # Get task type from form data
        task_type = request.form.get('task_type', None)
        print(f"Received file: {audio_file.filename}, Task type: {task_type}")
        
        # Save uploaded file temporarily with better file type handling
        file_extension = os.path.splitext(audio_file.filename)[1].lower()
        
        # Support all common audio formats
        supported_extensions = ['.wav', '.mp3', '.m4a', '.aac', '.ogg', '.flac', '.wma', '.mp4', '.mov', '.avi']
        
        if not file_extension or file_extension not in supported_extensions:
            # Default to wav if unknown extension
            file_extension = '.wav'
            print(f"Unknown extension, defaulting to .wav")
            
        print(f"Processing file: {audio_file.filename} with extension: {file_extension}")
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
            tmp_file_path = tmp_file.name
            audio_file.save(tmp_file_path)
            
            file_size = os.path.getsize(tmp_file_path)
            print(f"Saved temporary file: {tmp_file_path}, size: {file_size} bytes")
            
            if file_size == 0:
                print("Error: File is empty")
                return jsonify({'error': 'Uploaded file is empty'}), 400
            
            # Save recording for playback
            recordings_dir = os.path.join(os.getcwd(), 'recordings')
            os.makedirs(recordings_dir, exist_ok=True)
            
            # Generate unique filename with timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            saved_filename = f"recording_{timestamp}{file_extension}"
            saved_path = os.path.join(recordings_dir, saved_filename)
            
            # Copy the temp file to permanent location
            import shutil
            shutil.copy2(tmp_file_path, saved_path)
            print(f"Recording saved as: {saved_filename}")
            
            # Analyze the audio with task-specific analysis
            print("Starting audio analysis...")
            print(f"Current feature names: {len(analyzer.feature_names)} features")
            results = analyzer.analyze_audio(tmp_file_path, task_type)
            results['recording_filename'] = saved_filename
            results['recording_url'] = f'/recordings/{saved_filename}'
            print("Audio analysis completed successfully")
            
            # Convert numpy types to Python native types for JSON serialization
            def convert_numpy_types(obj):
                if isinstance(obj, dict):
                    return {key: convert_numpy_types(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.bool_):
                    return bool(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                else:
                    return obj
            
            # Clean results for JSON serialization
            clean_results = convert_numpy_types(results)
            
            return jsonify(clean_results)
    
    except Exception as e:
        error_msg = str(e)
        print(f"=== ERROR ANALYZING AUDIO ===")
        print(f"Error: {error_msg}")
        import traceback
        traceback.print_exc()
        print(f"=== END ERROR ===")
        
        # Return more specific error messages
        if "Could not load audio file" in error_msg:
            return jsonify({'error': 'Unable to load audio file. Please try a different format (WAV, MP3, etc.)'}), 400
        elif "Audio too short" in error_msg:
            return jsonify({'error': 'Audio file is too short. Please upload at least 1 second of audio.'}), 400
        else:
            return jsonify({'error': f'Audio processing failed: {error_msg}'}), 500
    
    finally:
        # Clean up temporary file
        if tmp_file_path and os.path.exists(tmp_file_path):
            try:
                os.unlink(tmp_file_path)
                print(f"Cleaned up temporary file: {tmp_file_path}")
            except Exception as cleanup_error:
                print(f"Warning: Could not clean up temp file: {cleanup_error}")

@app.route('/demo')
def demo():
    """Generate demo results for testing"""
    risk_score = np.random.uniform(10, 90)
    
    # Generate realistic transcript based on risk level
    transcript = analyzer.generate_realistic_transcript(risk_score)
    
    # Generate mock features
    features = {name: np.random.uniform(0, 1) for name in analyzer.feature_names}
    
    # Adjust features to match risk level
    if risk_score > 60:
        features['speaking_rate'] = np.random.uniform(60, 100)
        features['pause_rate'] = np.random.uniform(0.3, 0.6)
        features['type_token_ratio'] = np.random.uniform(0.2, 0.4)
    else:
        features['speaking_rate'] = np.random.uniform(100, 150)
        features['pause_rate'] = np.random.uniform(0.1, 0.3)
        features['type_token_ratio'] = np.random.uniform(0.4, 0.7)
    
    risk_assessment = analyzer._calculate_risk_assessment(risk_score, features)
    biomarkers = analyzer._generate_biomarker_report(features, risk_score)
    
    # Convert numpy types to Python native types for JSON serialization
    def convert_numpy_types(obj):
        if isinstance(obj, dict):
            return {key: convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj
    
    demo_results = {
        'transcript': transcript,
        'features': features,
        'risk_score': risk_score,
        'risk_assessment': risk_assessment,
        'biomarkers': biomarkers
    }
    
    return jsonify(convert_numpy_types(demo_results))

# Add route to serve saved recordings
@app.route('/recordings/<filename>')
def serve_recording(filename):
    """Serve saved recording files"""
    recordings_dir = os.path.join(os.getcwd(), 'recordings')
    return send_from_directory(recordings_dir, filename)

# Add static folder for images
app.static_folder = 'static'

if __name__ == '__main__':
    import sys
    port = 5001
    if len(sys.argv) > 1 and sys.argv[1] == '--port':
        port = int(sys.argv[2])
    app.run(debug=True, host='0.0.0.0', port=port)
