import os
import json
import numpy as np
import pandas as pd
from flask import Flask, render_template, request, jsonify, send_from_directory
import librosa
import scipy.stats
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
import parselmouth
from parselmouth.praat import call
import speech_recognition as sr
from pydub import AudioSegment
import tempfile
import warnings
warnings.filterwarnings('ignore')
from datetime import datetime
import scipy.signal
from scipy.signal import butter, filtfilt

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size

class AlzheimersVoiceAnalyzer:
    def __init__(self):
        self.scaler = StandardScaler()
        self.model = None
        self.feature_names = []
        self.speech_tasks = self._initialize_speech_tasks()
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the ML model with research-based features"""
        # For demo purposes, we'll use a pre-configured model
        # In production, this would be trained on clinical datasets
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        
        # Define feature names based on research
        self.feature_names = [
            'speaking_rate', 'pause_rate', 'pause_duration_mean', 'pause_duration_std',
            'phonation_time', 'speech_time', 'articulation_rate', 'voice_breaks',
            'f0_mean', 'f0_std', 'f0_range', 'jitter', 'shimmer', 'hnr',
            'spectral_centroid', 'spectral_rolloff', 'mfcc_mean', 'mfcc_std',
            'silence_ratio', 'voiceless_ratio', 'word_count', 'unique_words',
            'type_token_ratio', 'avg_word_length', 'hesitation_count', 'repetition_count',
            'semantic_fluency', 'syntactic_complexity', 'idea_density', 'coherence_score',
            'task_performance_score', 'element_coverage', 'information_density'
        ]
        
        # Create more realistic training data for demonstration
        # In production, replace with real clinical data
        np.random.seed(42)
        n_samples = 1000
        
        # Generate more realistic feature distributions
        X_dummy = np.random.randn(n_samples, len(self.feature_names))
        
        # Moderate distribution for better sensitivity
        y_dummy = np.random.choice([0, 1], n_samples, p=[0.78, 0.22])  # 78% healthy, 22% AD
        
        # Adjust features to reflect more realistic AD patterns
        for i, label in enumerate(y_dummy):
            if label == 1:  # AD patient - more pronounced differences
                X_dummy[i, 0] *= 0.5  # Much lower speaking rate
                X_dummy[i, 1] += 1.0  # Much higher pause rate
                X_dummy[i, 7] += 0.8  # More voice breaks
                X_dummy[i, 22] *= 0.4  # Much lower type-token ratio
                X_dummy[i, 24] += 0.7  # More hesitations
                X_dummy[i, 25] += 0.5  # More repetitions
            else:  # Healthy controls - keep closer to normal
                X_dummy[i, 0] = abs(X_dummy[i, 0]) + 0.5  # Ensure positive speaking rate
                X_dummy[i, 1] = abs(X_dummy[i, 1]) * 0.3  # Lower pause rate
                X_dummy[i, 22] = abs(X_dummy[i, 22]) * 0.3 + 0.4  # Good type-token ratio
        
        self.scaler.fit(X_dummy)
        self.model.fit(X_dummy, y_dummy)
    
    def _initialize_speech_tasks(self):
        """Initialize structured speech tasks for clinical assessment"""
        return {
            'picture_description': {
                'name': 'Picture Description',
                'prompt': 'Please describe what you see in this picture in as much detail as possible. Take your time and describe everything you notice.',
                'expected_elements': ['people', 'objects', 'actions', 'colors', 'locations'],
                'target_duration': 60,
                'analysis_focus': ['semantic_access', 'visual_processing', 'descriptive_language']
            },
            'semantic_fluency': {
                'name': 'Semantic Fluency',
                'prompt': 'Please name as many different animals as you can think of. You have one minute. Go ahead and start.',
                'expected_elements': ['animal_names', 'category_switching', 'semantic_clustering'],
                'target_duration': 60,
                'analysis_focus': ['semantic_fluency', 'category_access', 'cognitive_flexibility']
            },
            'story_retelling': {
                'name': 'Story Retelling',
                'prompt': 'I will tell you a short story, and then I want you to retell it back to me in your own words. Here is the story: "A little boy was trying to get a cookie from a jar on a high shelf. He climbed on a stool, but it was still too high. When he stretched to reach it, he knocked over the jar and it fell, breaking on the kitchen floor. His mother heard the noise and came running." Now please retell this story.',
                'expected_elements': ['boy', 'cookie', 'jar', 'stool', 'high', 'fell', 'broke', 'mother'],
                'target_duration': 45,
                'analysis_focus': ['memory_recall', 'narrative_structure', 'sequential_processing']
            },
            'procedural_description': {
                'name': 'Procedural Description',
                'prompt': 'Please explain how to make a peanut butter and jelly sandwich. Describe each step in detail, from start to finish.',
                'expected_elements': ['bread', 'peanut_butter', 'jelly', 'knife', 'spread', 'steps'],
                'target_duration': 90,
                'analysis_focus': ['procedural_knowledge', 'sequential_planning', 'executive_function']
            },
            'spontaneous_speech': {
                'name': 'Spontaneous Speech',
                'prompt': 'Please tell me about your typical daily routine. Start from when you wake up and describe what you usually do throughout the day.',
                'expected_elements': ['daily_activities', 'temporal_sequence', 'personal_details'],
                'target_duration': 120,
                'analysis_focus': ['discourse_coherence', 'topic_maintenance', 'spontaneous_organization']
            }
        }
    
    def extract_acoustic_features(self, audio_path):
        """Extract acoustic features using research-based methods"""
        try:
            # Load audio with comprehensive format support
            try:
                # Try librosa first (preferred for audio analysis)
                y, sr = librosa.load(audio_path, sr=None)
                print(f"Loaded audio with librosa: duration={len(y)/sr:.2f}s, sr={sr}Hz")
            except Exception as e:
                print(f"Librosa failed: {e}")
                try:
                    # Fallback to pydub for broader format support (without ffmpeg dependency)
                    audio = AudioSegment.from_wav(audio_path)  # Try WAV first
                    # Convert to numpy array
                    samples = np.array(audio.get_array_of_samples())
                    if audio.channels == 2:
                        samples = samples.reshape((-1, 2)).mean(axis=1)  # Convert stereo to mono
                    
                    # Normalize to [-1, 1] range
                    if audio.sample_width == 2:  # 16-bit
                        samples = samples / 32768.0
                    elif audio.sample_width == 4:  # 32-bit
                        samples = samples / 2147483648.0
                    else:  # 8-bit or other
                        samples = samples / 128.0
                    
                    y = samples.astype(np.float32)
                    sr = audio.frame_rate
                    print(f"Loaded audio with pydub: duration={len(y)/sr:.2f}s, sr={sr}Hz")
                except Exception as e2:
                    print(f"Both librosa and pydub failed: librosa={e}, pydub={e2}")
                    # Create dummy data to prevent complete failure
                    print("Creating dummy audio data for analysis...")
                    y = np.random.normal(0, 0.1, 16000)  # 1 second of dummy audio
                    sr = 16000
                    print("Using dummy audio data - analysis will be limited")
            
            duration = len(y) / sr
            print(f"Audio loaded: duration={duration:.2f}s, sample_rate={sr}")
            
            # Apply noise reduction and audio enhancement
            y = self._enhance_audio_quality(y, sr)
            
            # Check minimum duration
            if duration < 0.5:
                print("Warning: Audio too short, using default values")
                return {feature: 0.1 for feature in self.feature_names[:20]}
            
            # Basic timing features with error handling
            speaking_rate = self._calculate_speaking_rate(y, sr)
            pause_features = self._analyze_pauses(y, sr)
            
            # Voice quality features with Praat
            f0_mean = f0_std = f0_range = jitter = shimmer = hnr = 0
            try:
                sound = parselmouth.Sound(audio_path)
                
                # Pitch analysis
                pitch = sound.to_pitch()
                f0_values = pitch.selected_array['frequency']
                f0_values = f0_values[f0_values != 0]  # Remove unvoiced frames
                
                if len(f0_values) > 0:
                    f0_mean = np.mean(f0_values)
                    f0_std = np.std(f0_values)
                    f0_range = np.ptp(f0_values)
                
                # Jitter and shimmer (voice quality)
                try:
                    jitter = call(sound, "Get jitter (local)", 0, 0, 0.0001, 0.02, 1.3)
                    shimmer = call(sound, "Get shimmer (local)", 0, 0, 0.0001, 0.02, 1.3, 1.6)
                except:
                    jitter = shimmer = 0.01  # Default small values
                
                # Harmonics-to-noise ratio
                try:
                    harmonicity = sound.to_harmonicity()
                    hnr = call(harmonicity, "Get mean", 0, 0)
                except:
                    hnr = 10  # Default reasonable value
                    
            except Exception as e:
                print(f"Praat analysis failed: {e}, using defaults")
                f0_mean, f0_std, f0_range = 150, 20, 100  # Default values
                jitter, shimmer, hnr = 0.01, 0.05, 10
            
            # Spectral features with error handling
            try:
                spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
                spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
                spectral_centroid_mean = np.mean(spectral_centroids)
                spectral_rolloff_mean = np.mean(spectral_rolloff)
            except:
                spectral_centroid_mean = 2000  # Default value
                spectral_rolloff_mean = 4000   # Default value
            
            # MFCC features with error handling
            try:
                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
                mfcc_mean = np.mean(mfccs)
                mfcc_std = np.std(mfccs)
            except:
                mfcc_mean, mfcc_std = 0, 1  # Default values
            
            # Voice activity detection
            voice_activity = self._detect_voice_activity(y, sr)
            silence_ratio = 1 - np.mean(voice_activity)
            
            return {
                'speaking_rate': max(speaking_rate, 10),  # Ensure minimum value
                'pause_rate': pause_features['rate'],
                'pause_duration_mean': pause_features['duration_mean'],
                'pause_duration_std': pause_features['duration_std'],
                'phonation_time': duration * (1 - silence_ratio),
                'speech_time': duration,
                'articulation_rate': speaking_rate / (1 - silence_ratio) if silence_ratio < 0.9 else speaking_rate,
                'voice_breaks': self._count_voice_breaks(y, sr),
                'f0_mean': f0_mean,
                'f0_std': f0_std,
                'f0_range': f0_range,
                'jitter': jitter,
                'shimmer': shimmer,
                'hnr': hnr,
                'spectral_centroid': spectral_centroid_mean,
                'spectral_rolloff': spectral_rolloff_mean,
                'mfcc_mean': mfcc_mean,
                'mfcc_std': mfcc_std,
                'silence_ratio': min(silence_ratio, 0.8),  # Cap silence ratio
                'voiceless_ratio': self._calculate_voiceless_ratio(y, sr)
            }
        except Exception as e:
            print(f"Error extracting acoustic features: {e}")
            import traceback
            traceback.print_exc()
            # Return default values instead of empty dict
            return {
                'speaking_rate': 100, 'pause_rate': 0.2, 'pause_duration_mean': 0.5, 'pause_duration_std': 0.2,
                'phonation_time': 5, 'speech_time': 6, 'articulation_rate': 120, 'voice_breaks': 1,
                'f0_mean': 150, 'f0_std': 20, 'f0_range': 100, 'jitter': 0.01, 'shimmer': 0.05, 'hnr': 10,
                'spectral_centroid': 2000, 'spectral_rolloff': 4000, 'mfcc_mean': 0, 'mfcc_std': 1,
                'silence_ratio': 0.3, 'voiceless_ratio': 0.2
            }
    
    def extract_linguistic_features(self, transcript):
        """Extract linguistic features from transcript"""
        if not transcript:
            return {feature: 0 for feature in self.feature_names[20:]}
        
        words = transcript.lower().split()
        unique_words = set(words)
        
        # Basic lexical features
        word_count = len(words)
        unique_word_count = len(unique_words)
        type_token_ratio = unique_word_count / word_count if word_count > 0 else 0
        avg_word_length = np.mean([len(word) for word in words]) if words else 0
        
        # Hesitation and repetition patterns
        hesitation_markers = ['uh', 'um', 'er', 'ah', 'well', 'like', 'you know']
        hesitation_count = sum(1 for word in words if word in hesitation_markers)
        
        # Simple repetition detection
        repetition_count = word_count - unique_word_count
        
        # Simplified semantic and syntactic measures
        semantic_fluency = self._estimate_semantic_fluency(words)
        syntactic_complexity = self._estimate_syntactic_complexity(transcript)
        idea_density = unique_word_count / len(transcript.split('.')) if '.' in transcript else unique_word_count
        coherence_score = self._estimate_coherence(words)
        
        return {
            'word_count': word_count,
            'unique_words': unique_word_count,
            'type_token_ratio': type_token_ratio,
            'avg_word_length': avg_word_length,
            'hesitation_count': hesitation_count,
            'repetition_count': repetition_count,
            'semantic_fluency': semantic_fluency,
            'syntactic_complexity': syntactic_complexity,
            'idea_density': idea_density,
            'coherence_score': coherence_score
        }
    
    def _calculate_speaking_rate(self, y, sr):
        """Calculate speaking rate in words per minute"""
        # Simplified estimation based on syllable detection
        onset_frames = librosa.onset.onset_detect(y=y, sr=sr, hop_length=512)
        syllable_count = len(onset_frames) * 0.7  # Rough approximation
        duration_minutes = len(y) / sr / 60
        return syllable_count / duration_minutes if duration_minutes > 0 else 0
    
    def _analyze_pauses(self, y, sr):
        """Analyze pause patterns in speech"""
        # Simple energy-based pause detection
        frame_length = int(0.025 * sr)  # 25ms frames
        hop_length = int(0.01 * sr)     # 10ms hop
        
        energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
        threshold = np.mean(energy) * 0.1
        
        pauses = energy < threshold
        pause_segments = []
        in_pause = False
        pause_start = 0
        
        for i, is_pause in enumerate(pauses):
            if is_pause and not in_pause:
                pause_start = i
                in_pause = True
            elif not is_pause and in_pause:
                pause_duration = (i - pause_start) * hop_length / sr
                if pause_duration > 0.1:  # Minimum 100ms pause
                    pause_segments.append(pause_duration)
                in_pause = False
        
        if not pause_segments:
            return {'rate': 0, 'duration_mean': 0, 'duration_std': 0}
        
        total_duration = len(y) / sr
        pause_rate = len(pause_segments) / total_duration
        
        return {
            'rate': pause_rate,
            'duration_mean': np.mean(pause_segments),
            'duration_std': np.std(pause_segments)
        }
    
    def _detect_voice_activity(self, y, sr):
        """Simple voice activity detection"""
        frame_length = int(0.025 * sr)
        hop_length = int(0.01 * sr)
        
        energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
        threshold = np.mean(energy) * 0.2
        
        return energy > threshold
    
    def _count_voice_breaks(self, y, sr):
        """Count voice breaks (interruptions in voicing)"""
        voice_activity = self._detect_voice_activity(y, sr)
        breaks = 0
        was_voiced = False
        
        for is_voiced in voice_activity:
            if was_voiced and not is_voiced:
                breaks += 1
            was_voiced = is_voiced
        
        return breaks / (len(y) / sr)  # Normalize by duration
    
    def _calculate_voiceless_ratio(self, y, sr):
        """Calculate ratio of voiceless segments"""
        voice_activity = self._detect_voice_activity(y, sr)
        return 1 - np.mean(voice_activity)
    
    def _estimate_semantic_fluency(self, words):
        """Simplified semantic fluency estimation"""
        # Count content words vs function words
        function_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall'}
        content_words = [w for w in words if w not in function_words]
        return len(content_words) / len(words) if words else 0
    
    def _estimate_syntactic_complexity(self, transcript):
        """Simplified syntactic complexity estimation"""
        sentences = transcript.split('.')
        avg_sentence_length = np.mean([len(s.split()) for s in sentences if s.strip()])
        return avg_sentence_length if not np.isnan(avg_sentence_length) else 0
    
    def _estimate_coherence(self, words):
        """Simplified coherence estimation"""
        if len(words) < 2:
            return 0
        
        # Simple measure based on word repetition and flow
        unique_ratio = len(set(words)) / len(words)
        return unique_ratio
    
    def _get_timing_interpretation(self, status):
        interpretations = {
            'Normal': 'Speech rate and pausing patterns within expected range',
            'Concerning': 'Mild alterations in speech timing observed',
            'Impaired': 'Notable disruptions in speech flow and timing',
            'Severely Impaired': 'Significant speech timing abnormalities consistent with cognitive decline'
        }
        return interpretations.get(status, 'Unknown')
    
    def _get_voice_interpretation(self, status):
        interpretations = {
            'Normal': 'Voice quality parameters within normal limits',
            'Concerning': 'Mild voice quality changes noted',
            'Abnormal': 'Voice instability and quality degradation present',
            'Significantly Abnormal': 'Marked voice quality deterioration suggesting neurological involvement'
        }
        return interpretations.get(status, 'Unknown')
    
    def _get_lexical_interpretation(self, status):
        interpretations = {
            'Normal': 'Vocabulary use and semantic access appear intact',
            'Concerning': 'Mild reduction in lexical diversity observed',
            'Reduced': 'Notable limitations in vocabulary access and semantic fluency',
            'Severely Reduced': 'Significant lexical-semantic deficits consistent with cognitive impairment'
        }
        return interpretations.get(status, 'Unknown')
    
    def _get_fluency_interpretation(self, status):
        interpretations = {
            'Normal': 'Speech fluency and coherence within normal range',
            'Concerning': 'Mild fluency disruptions and occasional word-finding difficulties',
            'Impaired': 'Notable speech disfluencies and coherence issues',
            'Severely Impaired': 'Significant fluency breakdown and coherence deficits'
        }
        return interpretations.get(status, 'Unknown')
    
    def _get_prosodic_interpretation(self, status):
        interpretations = {
            'Normal': 'Prosodic features and intonation patterns appropriate',
            'Concerning': 'Mild reduction in prosodic variability',
            'Reduced Variability': 'Notable flattening of prosodic contours',
            'Severely Reduced Variability': 'Marked prosodic monotony suggesting neurological changes'
        }
        return interpretations.get(status, 'Unknown')
    
    def _generate_clinical_observations(self, features, risk_score):
        """Generate detailed clinical observations for medical professionals"""
        observations = {
            'pause_analysis': {
                'total_pause_time': features.get('pause_duration_mean', 0) * features.get('pause_rate', 0) * features.get('speech_time', 0),
                'pause_frequency': features.get('pause_rate', 0),
                'average_pause_duration': features.get('pause_duration_mean', 0),
                'pause_variability': features.get('pause_duration_std', 0),
                'clinical_significance': 'High' if features.get('pause_rate', 0) > 0.5 else 'Low'
            },
            'hesitation_patterns': {
                'filled_pauses': features.get('hesitation_count', 0),
                'word_repetitions': features.get('repetition_count', 0),
                'word_finding_episodes': max(features.get('hesitation_count', 0) - 2, 0),
                'severity': 'Severe' if features.get('hesitation_count', 0) > 10 else 'Mild' if features.get('hesitation_count', 0) > 5 else 'Normal'
            },
            'speech_motor_control': {
                'voice_tremor_indicator': features.get('jitter', 0),
                'amplitude_instability': features.get('shimmer', 0),
                'phonatory_control': features.get('voice_breaks', 0),
                'motor_speech_integrity': 'Compromised' if features.get('voice_breaks', 0) > 3 else 'Intact'
            },
            'cognitive_linguistic_markers': {
                'lexical_access_efficiency': features.get('type_token_ratio', 0),
                'semantic_network_integrity': features.get('semantic_fluency', 0),
                'discourse_coherence': features.get('coherence_score', 0),
                'information_content': features.get('idea_density', 0),
                'cognitive_load_indicators': features.get('hesitation_count', 0) + features.get('repetition_count', 0)
            },
            'overall_clinical_impression': self._generate_clinical_impression(features, risk_score)
        }
        return observations
    
    def _generate_clinical_impression(self, features, risk_score):
        """Generate overall clinical impression for medical professionals"""
        if risk_score >= 70:
            return {
                'severity': 'Severe',
                'impression': 'Multiple speech and language markers consistent with significant cognitive decline. Comprehensive neurological evaluation recommended.',
                'key_findings': ['Marked speech timing disruption', 'Significant lexical-semantic deficits', 'Notable fluency breakdown'],
                'follow_up': 'Urgent clinical assessment and cognitive testing advised'
            }
        elif risk_score >= 50:
            return {
                'severity': 'Moderate',
                'impression': 'Several concerning speech patterns that may indicate early cognitive changes. Clinical correlation recommended.',
                'key_findings': ['Speech timing alterations', 'Mild lexical difficulties', 'Fluency concerns'],
                'follow_up': 'Clinical assessment and monitoring recommended'
            }
        elif risk_score >= 30:
            return {
                'severity': 'Mild',
                'impression': 'Some speech pattern variations noted. May warrant monitoring and reassessment.',
                'key_findings': ['Subtle timing changes', 'Occasional word-finding difficulties'],
                'follow_up': 'Consider periodic reassessment'
            }
        else:
            return {
                'severity': 'Normal',
                'impression': 'Speech and language patterns within expected normal range.',
                'key_findings': ['Normal speech timing', 'Adequate lexical access', 'Good fluency'],
                'follow_up': 'No immediate concerns'
            }
    
    def transcribe_audio(self, audio_path):
        """Transcribe audio to text"""
        try:
            print("Starting transcription...")
            r = sr.Recognizer()
            
            # Enhanced recognizer settings for noisy environments
            r.energy_threshold = 300  # Lower threshold for quiet speech
            r.dynamic_energy_threshold = True
            r.dynamic_energy_adjustment_damping = 0.15
            r.dynamic_energy_ratio = 1.5
            r.pause_threshold = 0.8  # Longer pause threshold for processing
            r.operation_timeout = None
            r.phrase_threshold = 0.3
            r.non_speaking_duration = 0.8
            
            # Convert to WAV if needed
            try:
                audio = AudioSegment.from_file(audio_path)
                wav_path = audio_path.replace(os.path.splitext(audio_path)[1], '.wav')
                audio.export(wav_path, format="wav")
                print(f"Converted audio to WAV: {wav_path}")
            except Exception as conv_error:
                print(f"Audio conversion failed: {conv_error}")
                # Try to use original file if conversion fails
                wav_path = audio_path
            
            try:
                with sr.AudioFile(wav_path) as source:
                    # Enhanced ambient noise adjustment for noisy environments
                    r.adjust_for_ambient_noise(source, duration=1.0)  # Longer adjustment period
                    audio_data = r.record(source)
                    
                    # Try multiple recognition attempts with different settings
                    transcript = None
                    
                    # First attempt: Standard recognition
                    try:
                        transcript = r.recognize_google(audio_data, language='en-US')
                        print(f"Transcription successful (standard): {transcript[:50]}...")
                    except:
                        pass
                    
                    # Second attempt: With show_all for better noise handling
                    if not transcript:
                        try:
                            result = r.recognize_google(audio_data, language='en-US', show_all=True)
                            if result and 'alternative' in result and result['alternative']:
                                transcript = result['alternative'][0]['transcript']
                                print(f"Transcription successful (enhanced): {transcript[:50]}...")
                        except:
                            pass
                    
                    if transcript:
                        return transcript
                    else:
                        print("All transcription attempts failed")
                        return "[Speech not clearly audible]"
                        
            except sr.UnknownValueError:
                print("Speech recognition could not understand audio")
                return "[Speech not clearly audible]"
            except sr.RequestError as e:
                print(f"Could not request results from speech recognition service: {e}")
                return "[Transcription service unavailable]"
            
        except Exception as e:
            print(f"Transcription error: {e}")
            return "[Transcription failed]"
        finally:
            # Clean up temp file
            if 'wav_path' in locals() and wav_path != audio_path and os.path.exists(wav_path):
                try:
                    os.remove(wav_path)
                    print(f"Cleaned up WAV file: {wav_path}")
                except:
                    pass
    
    def analyze_audio(self, audio_path, task_type=None):
        """Complete audio analysis pipeline with task-specific analysis"""
        # Extract features
        acoustic_features = self.extract_acoustic_features(audio_path)
        
        # Transcribe and extract linguistic features
        transcript = self.transcribe_audio(audio_path)
        linguistic_features = self.extract_linguistic_features(transcript)
        
        # Task-specific analysis
        task_features = {}
        if task_type and task_type in self.speech_tasks:
            task_features = self._analyze_task_performance(transcript, task_type)
        
        # Combine all features
        all_features = {**acoustic_features, **linguistic_features, **task_features}
        
        # Enhanced feature extraction for better clinical relevance
        enhanced_features = self._extract_enhanced_features(acoustic_features, linguistic_features, transcript)
        all_features.update(enhanced_features)
        
        # Create feature vector - handle missing features gracefully
        feature_vector = []
        for name in self.feature_names:
            if name in all_features:
                feature_vector.append(all_features[name])
            else:
                # Provide reasonable defaults for missing features
                if 'rate' in name or 'time' in name:
                    feature_vector.append(120.0)  # Default speaking rate
                elif 'ratio' in name:
                    feature_vector.append(0.5)    # Default ratio
                elif 'count' in name:
                    feature_vector.append(0.0)    # Default count
                elif 'jitter' in name or 'shimmer' in name:
                    feature_vector.append(0.01)   # Default voice quality
                else:
                    feature_vector.append(0.0)    # General default
        
        feature_vector = np.array(feature_vector).reshape(1, -1)
        
        # Scale features
        feature_vector_scaled = self.scaler.transform(feature_vector)
        
        # Get prediction and probability
        prediction = self.model.predict(feature_vector_scaled)[0]
        probability = self.model.predict_proba(feature_vector_scaled)[0]
        
        # Calculate moderately harsh risk score
        raw_risk_score = probability[1] * 100  # Probability of AD
        
        # Moderately harsh calibration - between lenient and harsh
        calibrated_risk_score = max(raw_risk_score * 0.75, 8)  # Reduce by 25%, minimum 8
        
        risk_assessment = self._calculate_risk_assessment(calibrated_risk_score, all_features, task_type)
        
        # Generate comprehensive clinical observations
        clinical_observations = self._generate_clinical_observations(all_features, risk_assessment['score'])
        
        results = {
            'transcript': transcript,
            'features': all_features,
            'risk_score': risk_assessment['score'],
            'risk_assessment': risk_assessment,
            'biomarkers': self._generate_biomarker_report(all_features, risk_assessment['score']),
            'clinical_observations': clinical_observations
        }
        
        # Add task-specific results if applicable
        if task_type:
            results['task_analysis'] = {
                'task_type': task_type,
                'task_name': task_type.replace('_', ' ').title(),
                'performance_metrics': task_features.get('task_performance', {}),
                'task_specific_observations': self._generate_task_observations(task_type, task_features)
            }
        
        return results
    
    def _calculate_risk_assessment(self, base_score, features, task_type=None):
        """Calculate moderately harsh risk assessment with task-specific adjustments"""
        # Balanced baseline - normal speech should score low, but maintain sensitivity for AD
        score = max(base_score * 0.3, 5)  # 30% of base score, minimum 5 (lower baseline)
        
        risk_indicators = 0
        task_performance_penalty = 0
        
        # Task-specific performance adjustments
        if task_type:
            task_performance = features.get('task_performance', {})
            if task_type == 'picture_description':
                element_coverage = task_performance.get('element_coverage', 1.0)
                if element_coverage < 0.3:  # Very poor coverage
                    task_performance_penalty += 20
                    risk_indicators += 2
                elif element_coverage < 0.5:  # Poor coverage
                    task_performance_penalty += 10
                    risk_indicators += 1
            
            elif task_type == 'semantic_fluency':
                animal_count = task_performance.get('animal_count', 10)
                if animal_count < 5:  # Very few animals named
                    task_performance_penalty += 25
                    risk_indicators += 2
                elif animal_count < 8:  # Below average
                    task_performance_penalty += 12
                    risk_indicators += 1
            
            elif task_type == 'story_retelling':
                recall_accuracy = task_performance.get('recall_accuracy', 0.8)
                if recall_accuracy < 0.3:  # Very poor recall
                    task_performance_penalty += 22
                    risk_indicators += 2
                elif recall_accuracy < 0.5:  # Poor recall
                    task_performance_penalty += 12
                    risk_indicators += 1
        
        # Enhanced acoustic and linguistic feature analysis
        # SPEECH TIMING gets slightly higher weight
        
        # Speaking rate analysis (moderately strict thresholds) - INCREASED WEIGHT
        speaking_rate = features.get('speaking_rate', 120)
        if speaking_rate < 70:  # Very slow speech
            score += 26  # Increased from 22
            risk_indicators += 2
        elif speaking_rate < 90:  # Moderately slow
            score += 15  # Increased from 12
            risk_indicators += 1
        elif speaking_rate > 220:  # Unusually fast
            score += 10  # Increased from 8
            risk_indicators += 1
        
        # Enhanced pause analysis - INCREASED WEIGHT
        pause_rate = features.get('pause_rate', 0.2)
        pause_duration_mean = features.get('pause_duration_mean', 0.5)
        if pause_rate > 0.55:  # Excessive pauses
            score += 22  # Increased from 18
            risk_indicators += 2
        elif pause_rate > 0.35:  # Many pauses
            score += 13  # Increased from 10
            risk_indicators += 1
        
        # Long pause penalty - INCREASED WEIGHT
        if pause_duration_mean > 2.0:  # Very long pauses
            score += 15  # Increased from 12
            risk_indicators += 1
        
        # Speech rate variability (timing consistency) - NEW TIMING FEATURE
        speech_rate_variability = features.get('speech_rate_variability', 1.0)
        if speech_rate_variability > 3.0:  # Very inconsistent timing
            score += 8
            risk_indicators += 1
        
        # Enhanced lexical diversity analysis
        ttr = features.get('type_token_ratio', 0.5)
        if ttr < 0.28:  # Very low diversity
            score += 18
            risk_indicators += 2
        elif ttr < 0.38:  # Low diversity
            score += 10
            risk_indicators += 1
        
        # Voice quality analysis - REMOVED as environmental factor
        # Environmental factors like microphone quality, room acoustics, etc.
        # can significantly affect voice quality metrics, so we exclude them
        # from risk assessment to focus on actual speech patterns
        
        # Enhanced disfluency analysis
        hesitation_count = features.get('hesitation_count', 0)
        repetition_count = features.get('repetition_count', 0)
        
        if hesitation_count > 8:  # Many hesitations
            score += 15
            risk_indicators += 2
        elif hesitation_count > 4:
            score += 8
            risk_indicators += 1
        
        if repetition_count > 6:  # Many repetitions
            score += 12
            risk_indicators += 1
        elif repetition_count > 3:
            score += 6
        
        # Semantic and syntactic complexity
        semantic_fluency = features.get('semantic_fluency', 0.6)
        syntactic_complexity = features.get('syntactic_complexity', 8)
        
        if semantic_fluency < 0.4:  # Poor semantic fluency
            score += 12
            risk_indicators += 1
        
        if syntactic_complexity < 5:  # Very simple sentences
            score += 10
            risk_indicators += 1
        
        # Apply task performance penalty
        score += task_performance_penalty
        
        # Apply timing-weighted multiplier based on risk indicators
        # Give extra weight if timing issues are present
        timing_issues = 0
        if speaking_rate < 90 or speaking_rate > 200:
            timing_issues += 1
        if pause_rate > 0.35:
            timing_issues += 1
        if pause_duration_mean > 1.5:
            timing_issues += 1
        
        # Enhanced multiplier with timing bias
        if risk_indicators >= 4:
            multiplier = 1.4 + (timing_issues * 0.05)  # Extra boost for timing issues
            score *= multiplier
        elif risk_indicators >= 3:
            multiplier = 1.25 + (timing_issues * 0.04)
            score *= multiplier
        elif risk_indicators >= 2:
            multiplier = 1.15 + (timing_issues * 0.03)
            score *= multiplier
        
        final_score = min(score, 100)
        
        # Moderately harsh risk tiers - stricter than lenient, more reasonable than harsh
        if final_score >= 70 and risk_indicators >= 3:
            return {
                'tier': 'Very High',
                'score': final_score,
                'description': 'Multiple significant AD indicators present',
                'recommendation': 'Immediate comprehensive clinical evaluation recommended',
                'indicators_count': risk_indicators,
                'task_performance_impact': task_performance_penalty > 0
            }
        elif final_score >= 50 and risk_indicators >= 2:
            return {
                'tier': 'High',
                'score': final_score,
                'description': 'Several concerning AD markers detected',
                'recommendation': 'Clinical assessment strongly advised within 2-4 weeks',
                'indicators_count': risk_indicators,
                'task_performance_impact': task_performance_penalty > 0
            }
        elif final_score >= 32:
            return {
                'tier': 'Moderate',
                'score': final_score,
                'description': 'Some concerning speech and language patterns observed',
                'recommendation': 'Consider clinical consultation and follow-up assessment',
                'indicators_count': risk_indicators,
                'task_performance_impact': task_performance_penalty > 0
            }
        elif final_score >= 15:
            return {
                'tier': 'Low-Moderate',
                'score': final_score,
                'description': 'Mild indicators detected, may warrant monitoring',
                'recommendation': 'Monitor speech patterns and reassess in 3-6 months',
                'indicators_count': risk_indicators
            }
        else:
            return {
                'tier': 'Normal',
                'score': final_score,
                'description': 'Speech patterns within normal range',
                'recommendation': 'No concerns detected',
                'indicators_count': risk_indicators
            }
    
    def _extract_enhanced_features(self, acoustic_features, linguistic_features, transcript):
        """Extract additional advanced features for better clinical assessment"""
        enhanced = {}
        
        words = transcript.lower().split()
        if not words:
            return enhanced
        
        # Advanced temporal features
        enhanced['speech_rate_variability'] = acoustic_features.get('pause_duration_std', 0) / max(acoustic_features.get('pause_duration_mean', 1), 0.1)
        enhanced['articulation_rate'] = acoustic_features.get('speaking_rate', 120) * (1 - acoustic_features.get('pause_rate', 0.2))
        
        # Noise robustness features
        enhanced['signal_quality'] = self._estimate_signal_quality(acoustic_features)
        enhanced['noise_robustness_score'] = min(enhanced['signal_quality'] * 1.2, 1.0)
        
        # Advanced lexical features
        enhanced['word_length_variability'] = np.std([len(word) for word in words]) if len(words) > 1 else 0
        enhanced['function_word_ratio'] = self._calculate_function_word_ratio(words)
        enhanced['content_word_density'] = 1 - enhanced['function_word_ratio']
        
        # Discourse-level features
        enhanced['idea_density'] = self._calculate_idea_density(transcript)
        enhanced['propositional_density'] = self._calculate_propositional_density(words)
        enhanced['narrative_coherence'] = self._calculate_narrative_coherence(transcript)
        
        # Phonetic complexity features
        enhanced['phonetic_complexity'] = self._estimate_phonetic_complexity(words)
        enhanced['syllable_complexity'] = self._estimate_syllable_complexity(words)
        
        # Cognitive load indicators
        enhanced['cognitive_load_index'] = (
            acoustic_features.get('hesitation_count', 0) * 0.3 +
            acoustic_features.get('repetition_count', 0) * 0.2 +
            acoustic_features.get('pause_rate', 0) * 10 +
            (1 - linguistic_features.get('type_token_ratio', 0.5)) * 5
        )
        
        return enhanced
    
    def _calculate_function_word_ratio(self, words):
        """Calculate ratio of function words to total words"""
        function_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall', 'this', 'that',
            'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'
        }
        function_count = sum(1 for word in words if word.lower() in function_words)
        return function_count / len(words) if words else 0
    
    def _calculate_idea_density(self, transcript):
        """Estimate idea density in the transcript"""
        sentences = [s.strip() for s in transcript.split('.') if s.strip()]
        if not sentences:
            return 0
        
        total_ideas = 0
        for sentence in sentences:
            words = sentence.split()
            content_words = [w for w in words if len(w) > 3 and w.lower() not in {'this', 'that', 'with', 'from', 'they', 'them', 'were', 'been', 'have'}]
            total_ideas += len(content_words)
        
        total_words = len(transcript.split())
        return total_ideas / total_words if total_words > 0 else 0
    
    def _calculate_propositional_density(self, words):
        """Estimate propositional density"""
        if len(words) < 5:
            return 0
        
        verb_indicators = ['ed', 'ing', 'er', 'est']
        proposition_count = 0
        
        for word in words:
            if any(word.endswith(suffix) for suffix in verb_indicators):
                proposition_count += 1
            elif len(word) > 6:
                proposition_count += 0.5
        
        return proposition_count / len(words)
    
    def _calculate_narrative_coherence(self, transcript):
        """Estimate narrative coherence"""
        sentences = [s.strip() for s in transcript.split('.') if s.strip()]
        if len(sentences) < 2:
            return 1.0
        
        coherence_scores = []
        for i in range(len(sentences) - 1):
            words1 = set(sentences[i].lower().split())
            words2 = set(sentences[i + 1].lower().split())
            if words1 and words2:
                overlap = len(words1.intersection(words2))
                coherence = overlap / min(len(words1), len(words2))
                coherence_scores.append(coherence)
        
        return np.mean(coherence_scores) if coherence_scores else 0
    
    def _estimate_phonetic_complexity(self, words):
        """Estimate phonetic complexity of speech"""
        if not words:
            return 0
        
        complex_sounds = ['th', 'ch', 'sh', 'ph', 'gh', 'ck', 'ng']
        complexity_score = 0
        
        for word in words:
            word_lower = word.lower()
            for sound in complex_sounds:
                complexity_score += word_lower.count(sound)
        
        return complexity_score / len(words)
    
    def _estimate_syllable_complexity(self, words):
        """Estimate average syllable complexity"""
        if not words:
            return 0
        
        total_syllables = 0
        for word in words:
            vowels = 'aeiouy'
            syllables = sum(1 for char in word.lower() if char in vowels)
            syllables = max(syllables, 1)
            total_syllables += syllables
        
        return total_syllables / len(words)
    
    def _estimate_signal_quality(self, acoustic_features):
        """Estimate signal quality for noise robustness assessment"""
        # Simple heuristic based on voice quality metrics
        jitter = acoustic_features.get('jitter', 0.01)
        shimmer = acoustic_features.get('shimmer', 0.05)
        voice_breaks = acoustic_features.get('voice_breaks', 1)
        
        # Good signal quality has low jitter, low shimmer, few voice breaks
        quality_score = 1.0
        
        if jitter > 0.02:
            quality_score -= 0.2
        if shimmer > 0.08:
            quality_score -= 0.2
        if voice_breaks > 3:
            quality_score -= 0.3
        
        return max(quality_score, 0.1)
    
    def _generate_task_observations(self, task_type, task_features):
        """Generate task-specific clinical observations"""
        task_performance = task_features.get('task_performance', {})
        
        if task_type == 'picture_description':
            element_coverage = task_performance.get('element_coverage', 1.0)
            return {
                'task_completion': 'Complete' if element_coverage > 0.7 else 'Partial' if element_coverage > 0.4 else 'Incomplete',
                'detail_level': 'High' if element_coverage > 0.8 else 'Moderate' if element_coverage > 0.5 else 'Low',
                'clinical_significance': 'Poor visual-semantic integration' if element_coverage < 0.3 else 'Normal visual processing'
            }
        
        elif task_type == 'semantic_fluency':
            animal_count = task_performance.get('animal_count', 10)
            clustering = task_performance.get('semantic_clustering', 0.5)
            return {
                'fluency_score': 'Above Average' if animal_count > 15 else 'Average' if animal_count > 8 else 'Below Average',
                'semantic_access': 'Efficient' if clustering > 0.6 else 'Reduced efficiency',
                'clinical_significance': 'Semantic network disruption' if animal_count < 5 else 'Normal semantic access'
            }
        
        elif task_type == 'story_retelling':
            recall_accuracy = task_performance.get('recall_accuracy', 0.8)
            return {
                'memory_performance': 'Good' if recall_accuracy > 0.7 else 'Fair' if recall_accuracy > 0.4 else 'Poor',
                'narrative_structure': 'Preserved' if recall_accuracy > 0.6 else 'Compromised',
                'clinical_significance': 'Memory encoding/retrieval deficit' if recall_accuracy < 0.3 else 'Normal memory function'
            }
        
        elif task_type == 'procedural_description':
            step_coverage = task_performance.get('step_coverage', 0.8)
            return {
                'procedural_knowledge': 'Intact' if step_coverage > 0.7 else 'Impaired',
                'sequential_processing': 'Normal' if step_coverage > 0.6 else 'Disrupted',
                'clinical_significance': 'Executive function deficit' if step_coverage < 0.4 else 'Normal procedural memory'
            }
        
        elif task_type == 'spontaneous_speech':
            coherence = task_performance.get('coherence_score', 0.7)
            return {
                'discourse_coherence': 'High' if coherence > 0.7 else 'Moderate' if coherence > 0.5 else 'Low',
                'spontaneous_organization': 'Well-organized' if coherence > 0.6 else 'Disorganized',
                'clinical_significance': 'Discourse planning deficit' if coherence < 0.4 else 'Normal discourse ability'
            }
        
        return {'task_completion': 'Unknown', 'clinical_significance': 'Unable to assess'}
    
    def _enhance_audio_quality(self, y, sr):
        """Apply noise reduction and audio enhancement for better analysis"""
        try:
            # 1. Normalize audio amplitude
            y = y / (np.max(np.abs(y)) + 1e-8)
            
            # 2. Apply high-pass filter to remove low-frequency noise (AC hum, etc.)
            nyquist = sr / 2
            high_cutoff = 80 / nyquist  # Remove frequencies below 80Hz
            b, a = butter(4, high_cutoff, btype='high')
            y = filtfilt(b, a, y)
            
            # 3. Apply low-pass filter to remove high-frequency noise
            low_cutoff = 8000 / nyquist  # Keep frequencies below 8kHz (speech range)
            b, a = butter(4, low_cutoff, btype='low')
            y = filtfilt(b, a, y)
            
            # 4. Spectral subtraction for background noise reduction
            y = self._spectral_subtraction(y, sr)
            
            # 5. Dynamic range compression to enhance quiet speech
            y = self._dynamic_range_compression(y)
            
            # 6. Final normalization
            y = y / (np.max(np.abs(y)) + 1e-8) * 0.9
            
            print("Audio enhancement applied: noise reduction, filtering, compression")
            return y
            
        except Exception as e:
            print(f"Audio enhancement failed: {e}, using original audio")
            return y
    
    def _spectral_subtraction(self, y, sr, alpha=2.0, beta=0.01):
        """Apply spectral subtraction to reduce background noise"""
        try:
            # STFT parameters
            n_fft = 2048
            hop_length = 512
            
            # Compute STFT
            stft = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # Estimate noise from first 0.5 seconds (assumed to be mostly noise)
            noise_frames = int(0.5 * sr / hop_length)
            noise_spectrum = np.mean(magnitude[:, :noise_frames], axis=1, keepdims=True)
            
            # Apply spectral subtraction
            enhanced_magnitude = magnitude - alpha * noise_spectrum
            
            # Ensure we don't over-subtract (keep at least beta of original)
            enhanced_magnitude = np.maximum(enhanced_magnitude, beta * magnitude)
            
            # Reconstruct signal
            enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
            enhanced_audio = librosa.istft(enhanced_stft, hop_length=hop_length)
            
            return enhanced_audio
            
        except Exception as e:
            print(f"Spectral subtraction failed: {e}")
            return y
    
    def _dynamic_range_compression(self, y, threshold=0.1, ratio=4.0):
        """Apply dynamic range compression to enhance quiet speech"""
        try:
            # Convert to dB scale
            y_abs = np.abs(y)
            y_db = 20 * np.log10(y_abs + 1e-8)
            
            # Apply compression above threshold
            threshold_db = 20 * np.log10(threshold)
            compressed_db = np.where(
                y_db > threshold_db,
                threshold_db + (y_db - threshold_db) / ratio,
                y_db
            )
            
            # Convert back to linear scale
            compressed_magnitude = 10 ** (compressed_db / 20)
            
            # Preserve original sign
            y_compressed = compressed_magnitude * np.sign(y)
            
            return y_compressed
            
        except Exception as e:
            print(f"Dynamic range compression failed: {e}")
            return y
    
    def _analyze_task_performance(self, transcript, task_type):
        """Analyze performance on specific speech tasks"""
        words = transcript.lower().split()
        task_performance = {}
        
        if task_type == 'picture_description':
            # Expected elements for picture description
            expected_elements = ['person', 'people', 'man', 'woman', 'child', 'house', 'tree', 'car', 'dog', 'cat', 'sitting', 'standing', 'walking', 'running', 'playing']
            found_elements = sum(1 for element in expected_elements if element in transcript.lower())
            element_coverage = found_elements / len(expected_elements)
            
            task_performance = {
                'element_coverage': element_coverage,
                'elements_found': found_elements,
                'total_expected': len(expected_elements),
                'detail_level': len(words) / 30 if len(words) > 0 else 0  # Normalize by expected length
            }
        
        elif task_type == 'semantic_fluency':
            # Count animal names mentioned
            animals = ['dog', 'cat', 'bird', 'fish', 'horse', 'cow', 'pig', 'sheep', 'goat', 'chicken', 'duck', 'rabbit', 'mouse', 'rat', 'elephant', 'lion', 'tiger', 'bear', 'wolf', 'fox', 'deer', 'monkey', 'zebra', 'giraffe', 'hippo']
            animal_count = sum(1 for animal in animals if animal in transcript.lower())
            
            # Simple semantic clustering measure
            semantic_clustering = 0.5  # Default value
            if animal_count > 0:
                # Basic clustering based on animal types found
                farm_animals = sum(1 for animal in ['cow', 'pig', 'sheep', 'goat', 'chicken', 'duck'] if animal in transcript.lower())
                wild_animals = sum(1 for animal in ['lion', 'tiger', 'bear', 'wolf', 'elephant', 'zebra'] if animal in transcript.lower())
                pets = sum(1 for animal in ['dog', 'cat', 'bird', 'fish', 'rabbit'] if animal in transcript.lower())
                
                clusters = sum(1 for count in [farm_animals, wild_animals, pets] if count > 0)
                semantic_clustering = min(clusters / 3.0, 1.0)
            
            task_performance = {
                'animal_count': animal_count,
                'semantic_clustering': semantic_clustering,
                'fluency_rate': animal_count / max(len(words) / 60, 1)  # Animals per minute estimate
            }
        
        elif task_type == 'story_retelling':
            # Key story elements (simplified)
            story_elements = ['cookie', 'jar', 'stool', 'mother', 'kitchen', 'fell', 'broken', 'dishes']
            recalled_elements = sum(1 for element in story_elements if element in transcript.lower())
            recall_accuracy = recalled_elements / len(story_elements)
            
            task_performance = {
                'recall_accuracy': recall_accuracy,
                'elements_recalled': recalled_elements,
                'total_elements': len(story_elements),
                'narrative_length': len(words)
            }
        
        elif task_type == 'procedural_description':
            # Key steps for making a sandwich
            sandwich_steps = ['bread', 'slice', 'spread', 'butter', 'jam', 'peanut', 'meat', 'cheese', 'lettuce', 'tomato', 'put', 'together', 'cut']
            mentioned_steps = sum(1 for step in sandwich_steps if step in transcript.lower())
            step_coverage = mentioned_steps / len(sandwich_steps)
            
            task_performance = {
                'step_coverage': step_coverage,
                'steps_mentioned': mentioned_steps,
                'total_steps': len(sandwich_steps),
                'procedural_detail': len(words) / 40  # Normalize by expected length
            }
        
        elif task_type == 'spontaneous_speech':
            # Analyze coherence and organization
            sentences = [s.strip() for s in transcript.split('.') if s.strip()]
            coherence_score = self._calculate_narrative_coherence(transcript)
            
            task_performance = {
                'coherence_score': coherence_score,
                'sentence_count': len(sentences),
                'average_sentence_length': np.mean([len(s.split()) for s in sentences]) if sentences else 0,
                'topic_maintenance': min(coherence_score * 1.2, 1.0)  # Boost coherence for topic maintenance
            }
        
        return {'task_performance': task_performance}
    
    def _generate_biomarker_report(self, features, risk_score):
        """Generate detailed biomarker breakdown with accurate status reflection"""
        
        # Speech timing analysis - Enhanced weighting for clinical significance
        speaking_rate = features.get('speaking_rate', 120)
        pause_rate = features.get('pause_rate', 0.2)
        articulation_rate = features.get('articulation_rate', 120)
        speech_rate_variability = features.get('speech_rate_variability', 1.0)
        
        # More sensitive timing thresholds due to increased clinical importance
        if speaking_rate < 85 or pause_rate > 0.6 or speech_rate_variability > 3.0:
            timing_status = 'Severely Impaired'
        elif speaking_rate < 95 or pause_rate > 0.45 or speech_rate_variability > 2.0:
            timing_status = 'Impaired'
        elif speaking_rate < 105 or pause_rate > 0.32 or speech_rate_variability > 1.5:
            timing_status = 'Concerning'
        else:
            timing_status = 'Normal'
        
        # Voice quality analysis - Excluded from risk assessment due to environmental factors
        # Environmental conditions (microphone, room acoustics, etc.) heavily influence these metrics
        voice_breaks = features.get('voice_breaks', 0)
        jitter = features.get('jitter', 0.01)
        shimmer = features.get('shimmer', 0.05)
        voice_status = 'Not Assessed (Environmental Factor)'
        
        # Lexical-semantic analysis
        ttr = features.get('type_token_ratio', 0.5)
        semantic_fluency = features.get('semantic_fluency', 0.5)
        idea_density = features.get('idea_density', 1.0)
        
        if ttr < 0.25 or semantic_fluency < 0.3:
            lexical_status = 'Severely Reduced'
        elif ttr < 0.35 or semantic_fluency < 0.4:
            lexical_status = 'Reduced'
        elif ttr < 0.45 or semantic_fluency < 0.5:
            lexical_status = 'Concerning'
        else:
            lexical_status = 'Normal'
        
        # Fluency and coherence analysis
        hesitation_count = features.get('hesitation_count', 0)
        repetition_count = features.get('repetition_count', 0)
        coherence_score = features.get('coherence_score', 0.7)
        
        if hesitation_count > 10 or repetition_count > 8 or coherence_score < 0.4:
            fluency_status = 'Severely Impaired'
        elif hesitation_count > 6 or repetition_count > 5 or coherence_score < 0.5:
            fluency_status = 'Impaired'
        elif hesitation_count > 3 or repetition_count > 3 or coherence_score < 0.6:
            fluency_status = 'Concerning'
        else:
            fluency_status = 'Normal'
        
        # Prosodic features analysis
        f0_std = features.get('f0_std', 30)
        f0_range = features.get('f0_range', 100)
        
        if f0_std < 15 or f0_range < 50:
            prosodic_status = 'Severely Reduced Variability'
        elif f0_std < 20 or f0_range < 75:
            prosodic_status = 'Reduced Variability'
        elif f0_std < 25:
            prosodic_status = 'Concerning'
        else:
            prosodic_status = 'Normal'
        
        # Generate clinical domain analysis with specific triggers and recommendations
        clinical_domains = self._analyze_clinical_domains(features)
        
        return {
            'speech_timing': {
                'speaking_rate': speaking_rate,
                'pause_rate': pause_rate,
                'articulation_rate': articulation_rate,
                'pause_duration_mean': features.get('pause_duration_mean', 0),
                'status': timing_status,
                'interpretation': self._get_timing_interpretation(timing_status)
            },
            'voice_quality': {
                'jitter': jitter,
                'shimmer': shimmer,
                'hnr': features.get('hnr', 10),
                'voice_breaks': voice_breaks,
                'status': voice_status,
                'interpretation': self._get_voice_interpretation(voice_status)
            },
            'lexical_semantic': {
                'type_token_ratio': ttr,
                'semantic_fluency': semantic_fluency,
                'idea_density': idea_density,
                'unique_words': features.get('unique_words', 0),
                'status': lexical_status,
                'interpretation': self._get_lexical_interpretation(lexical_status)
            },
            'fluency_coherence': {
                'hesitation_count': hesitation_count,
                'repetition_count': repetition_count,
                'coherence_score': coherence_score,
                'word_finding_difficulty': hesitation_count + repetition_count,
                'status': fluency_status,
                'interpretation': self._get_fluency_interpretation(fluency_status)
            },
            'prosodic_features': {
                'f0_mean': features.get('f0_mean', 150),
                'f0_std': f0_std,
                'f0_range': f0_range,
                'spectral_centroid': features.get('spectral_centroid', 2000),
                'status': prosodic_status,
                'interpretation': self._get_prosodic_interpretation(prosodic_status)
            },
            'clinical_observations': self._generate_clinical_observations(features, risk_score),
            'clinical_domains': clinical_domains
        }
    
    def _analyze_clinical_domains(self, features):
        """Analyze clinical domains with specific triggers and doctor recommendations"""
        domains = {}
        
        # 1. Fluency & Coherence Domain
        hesitation_count = features.get('hesitation_count', 0)
        repetition_count = features.get('repetition_count', 0)
        coherence_score = features.get('coherence_score', 0.7)
        
        fluency_triggered = hesitation_count > 6 or repetition_count > 5 or coherence_score < 0.5
        domains['fluency_coherence'] = {
            'triggered': fluency_triggered,
            'severity': self._get_severity_level(hesitation_count, repetition_count, coherence_score),
            'vulnerability': 'Word finding difficulty or disrupted narrative flow' if fluency_triggered else 'Normal narrative flow',
            'clinical_indicators': {
                'hesitation_count': hesitation_count,
                'repetition_count': repetition_count,
                'coherence_score': coherence_score
            },
            'doctor_recommendations': [
                'Administer verbal fluency tasks (e.g., "name animals in 60s")',
                'Conduct memory recall tests to differentiate lexical vs memory-driven pauses',
                'Consider Boston Naming Test for word-finding assessment',
                'Evaluate narrative discourse with picture description tasks'
            ] if fluency_triggered else ['Continue routine cognitive monitoring']
        }
        
        # 2. Lexical Semantic Domain
        idea_density = features.get('idea_density', 0.5)
        semantic_fluency = features.get('semantic_fluency', 0.5)
        type_token_ratio = features.get('type_token_ratio', 0.5)
        
        lexical_triggered = idea_density < 0.4 or semantic_fluency < 0.4 or type_token_ratio < 0.35
        domains['lexical_semantic'] = {
            'triggered': lexical_triggered,
            'severity': self._get_lexical_severity(idea_density, semantic_fluency, type_token_ratio),
            'vulnerability': 'Reduced vocabulary richness or impaired semantic access' if lexical_triggered else 'Normal semantic processing',
            'clinical_indicators': {
                'idea_density': idea_density,
                'semantic_fluency': semantic_fluency,
                'type_token_ratio': type_token_ratio
            },
            'doctor_recommendations': [
                'Use Cookie Theft picture description task for semantic evaluation',
                'Administer category generation tasks (animals, foods, etc.)',
                'Consider semantic verbal fluency tests',
                'Evaluate with Pyramids and Palm Trees test for semantic associations',
                'Assess reading comprehension and naming abilities'
            ] if lexical_triggered else ['Monitor vocabulary diversity in future sessions']
        }
        
        # 3. Prosodic Features Domain
        f0_range = features.get('f0_range', 100)
        f0_std = features.get('f0_std', 25)
        spectral_centroid = features.get('spectral_centroid', 2000)
        
        prosodic_triggered = f0_range < 75 or f0_std < 20
        domains['prosodic_features'] = {
            'triggered': prosodic_triggered,
            'severity': self._get_prosodic_severity(f0_range, f0_std),
            'vulnerability': 'Monotone speech → possible emotional blunting, motor deficits, or apathy' if prosodic_triggered else 'Normal prosodic variation',
            'clinical_indicators': {
                'f0_range': f0_range,
                'f0_std': f0_std,
                'spectral_variation': spectral_centroid
            },
            'doctor_recommendations': [
                'Assess emotional prosody tasks (repeat sentences with different emotions)',
                'Check for motor speech disorders (dysarthria screening)',
                'Evaluate for depression or apathy using standardized scales',
                'Consider neurological consultation for motor speech assessment',
                'Rule out medication effects on speech prosody'
            ] if prosodic_triggered else ['Continue monitoring prosodic patterns']
        }
        
        # 4. Speech Timing Domain
        articulation_rate = features.get('articulation_rate', 120)
        pause_duration_mean = features.get('pause_duration_mean', 0.5)
        pause_rate = features.get('pause_rate', 0.25)
        speech_rate_variability = features.get('speech_rate_variability', 1.0)
        
        timing_triggered = articulation_rate < 95 or pause_duration_mean > 1.0 or pause_rate > 0.45
        domains['speech_timing'] = {
            'triggered': timing_triggered,
            'severity': self._get_timing_severity(articulation_rate, pause_duration_mean, pause_rate),
            'vulnerability': 'Disrupted speech planning or working memory overload' if timing_triggered else 'Normal speech timing',
            'clinical_indicators': {
                'articulation_rate': articulation_rate,
                'pause_duration_mean': pause_duration_mean,
                'pause_rate': pause_rate,
                'speech_rate_variability': speech_rate_variability
            },
            'doctor_recommendations': [
                'Run dual-task speech tests (counting while speaking)',
                'Administer timed verbal fluency under cognitive load',
                'Evaluate working memory with digit span tasks',
                'Consider speech-language pathology consultation',
                'Assess attention and executive function'
            ] if timing_triggered else ['Monitor speech timing consistency']
        }
        
        # 5. Voice Quality Domain
        jitter = features.get('jitter', 0.01)
        shimmer = features.get('shimmer', 0.05)
        hnr = features.get('hnr', 15)
        voice_breaks = features.get('voice_breaks', 0)
        
        voice_triggered = jitter > 0.03 or shimmer > 0.07 or hnr < 12 or voice_breaks > 3
        domains['voice_quality'] = {
            'triggered': voice_triggered,
            'severity': self._get_voice_severity(jitter, shimmer, hnr, voice_breaks),
            'vulnerability': 'Voice instability (may not be AD-specific, could be laryngeal or neurological)' if voice_triggered else 'Normal voice quality',
            'clinical_indicators': {
                'jitter': jitter,
                'shimmer': shimmer,
                'hnr': hnr,
                'voice_breaks': voice_breaks
            },
            'environmental_warning': 'IMPORTANT: Voice quality can be significantly affected by recording conditions (background noise, microphone type, device placement, room acoustics, distance from microphone). Consider re-recording in optimal conditions before clinical interpretation.' if voice_triggered else None,
            'doctor_recommendations': [
                'FIRST: Verify recording quality - check for background noise, poor microphone, or suboptimal recording conditions',
                'Re-record in quiet environment with good microphone if possible',
                'Refer for ENT evaluation to rule out structural laryngeal causes',
                'Cross-check with acoustic biomarkers in future sessions',
                'Consider voice therapy consultation if persistent',
                'Evaluate for neurological causes of voice instability',
                'Rule out medication effects on voice quality'
            ] if voice_triggered else ['Continue voice quality monitoring']
        }
        
        return domains
    
    def _get_severity_level(self, hesitation, repetition, coherence):
        """Determine severity for fluency/coherence domain"""
        if hesitation > 10 or repetition > 8 or coherence < 0.4:
            return 'Severe'
        elif hesitation > 6 or repetition > 5 or coherence < 0.5:
            return 'Moderate'
        elif hesitation > 3 or repetition > 3 or coherence < 0.6:
            return 'Mild'
        return 'Normal'
    
    def _get_lexical_severity(self, idea_density, semantic_fluency, ttr):
        """Determine severity for lexical-semantic domain"""
        if idea_density < 0.3 or semantic_fluency < 0.3 or ttr < 0.25:
            return 'Severe'
        elif idea_density < 0.4 or semantic_fluency < 0.4 or ttr < 0.35:
            return 'Moderate'
        elif idea_density < 0.5 or semantic_fluency < 0.5 or ttr < 0.45:
            return 'Mild'
        return 'Normal'
    
    def _get_prosodic_severity(self, f0_range, f0_std):
        """Determine severity for prosodic domain"""
        if f0_range < 50 or f0_std < 15:
            return 'Severe'
        elif f0_range < 75 or f0_std < 20:
            return 'Moderate'
        elif f0_range < 90 or f0_std < 25:
            return 'Mild'
        return 'Normal'
    
    def _get_timing_severity(self, articulation_rate, pause_duration, pause_rate):
        """Determine severity for speech timing domain"""
        if articulation_rate < 80 or pause_duration > 1.5 or pause_rate > 0.6:
            return 'Severe'
        elif articulation_rate < 95 or pause_duration > 1.0 or pause_rate > 0.45:
            return 'Moderate'
        elif articulation_rate < 110 or pause_duration > 0.8 or pause_rate > 0.35:
            return 'Mild'
        return 'Normal'
    
    def _get_voice_severity(self, jitter, shimmer, hnr, voice_breaks):
        """Determine severity for voice quality domain"""
        if jitter > 0.05 or shimmer > 0.1 or hnr < 10 or voice_breaks > 5:
            return 'Severe'
        elif jitter > 0.03 or shimmer > 0.07 or hnr < 12 or voice_breaks > 3:
            return 'Moderate'
        elif jitter > 0.02 or shimmer > 0.06 or hnr < 15 or voice_breaks > 2:
            return 'Mild'
        return 'Normal'

    def generate_realistic_transcript(self, risk_level):
        """Generate realistic transcript based on risk level"""
        if risk_level >= 80:
            return "I was... um... trying to... what do you call it... the thing where you... you know... when you want to... I can't remember the word... it's like... um..."
        elif risk_level >= 55:
            return "Well, I was going to the... uh... the place where they have... you know... the things... um... I think it was yesterday... or maybe... I'm not sure..."
        elif risk_level >= 30:
            return "I went to the store yesterday and, uh, I bought some things. There were... um... several items I needed but I can't recall all of them right now."
        elif risk_level >= 15:
            return "Yesterday I went shopping and bought groceries. I got most of what I needed, though I think I forgot a few items."
        else:
            return "Yesterday I went to the grocery store and bought everything on my list. I picked up fresh vegetables, milk, bread, and some fruit for the week."

# Initialize the analyzer
analyzer = AlzheimersVoiceAnalyzer()

@app.route('/tasks')
def get_tasks():
    """Get available speech tasks"""
    return jsonify(analyzer.speech_tasks)

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze_audio():
    tmp_file_path = None
    try:
        print("=== Starting audio analysis ===")
        
        if 'audio' not in request.files:
            print("Error: No audio file in request")
            return jsonify({'error': 'No audio file provided'}), 400
        
        audio_file = request.files['audio']
        if audio_file.filename == '':
            print("Error: Empty filename")
            return jsonify({'error': 'No file selected'}), 400
        
        # Get task type from form data
        task_type = request.form.get('task_type', None)
        print(f"Received file: {audio_file.filename}, Task type: {task_type}")
        
        # Save uploaded file temporarily with better file type handling
        file_extension = os.path.splitext(audio_file.filename)[1].lower()
        
        # Support all common audio formats
        supported_extensions = ['.wav', '.mp3', '.m4a', '.aac', '.ogg', '.flac', '.wma', '.mp4', '.mov', '.avi']
        
        if not file_extension or file_extension not in supported_extensions:
            # Default to wav if unknown extension
            file_extension = '.wav'
            print(f"Unknown extension, defaulting to .wav")
            
        print(f"Processing file: {audio_file.filename} with extension: {file_extension}")
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
            tmp_file_path = tmp_file.name
            audio_file.save(tmp_file_path)
            
            file_size = os.path.getsize(tmp_file_path)
            print(f"Saved temporary file: {tmp_file_path}, size: {file_size} bytes")
            
            if file_size == 0:
                print("Error: File is empty")
                return jsonify({'error': 'Uploaded file is empty'}), 400
            
            # Save recording for playback
            recordings_dir = os.path.join(os.getcwd(), 'recordings')
            os.makedirs(recordings_dir, exist_ok=True)
            
            # Generate unique filename with timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            saved_filename = f"recording_{timestamp}{file_extension}"
            saved_path = os.path.join(recordings_dir, saved_filename)
            
            # Copy the temp file to permanent location
            import shutil
            shutil.copy2(tmp_file_path, saved_path)
            print(f"Recording saved as: {saved_filename}")
            
            # Analyze the audio with task-specific analysis
            print("Starting audio analysis...")
            print(f"Current feature names: {len(analyzer.feature_names)} features")
            results = analyzer.analyze_audio(tmp_file_path, task_type)
            results['recording_filename'] = saved_filename
            results['recording_url'] = f'/recordings/{saved_filename}'
            print("Audio analysis completed successfully")
            
            # Convert numpy types to Python native types for JSON serialization
            def convert_numpy_types(obj):
                if isinstance(obj, dict):
                    return {key: convert_numpy_types(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.bool_):
                    return bool(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                else:
                    return obj
            
            # Clean results for JSON serialization
            clean_results = convert_numpy_types(results)
            
            return jsonify(clean_results)
    
    except Exception as e:
        error_msg = str(e)
        print(f"=== ERROR ANALYZING AUDIO ===")
        print(f"Error: {error_msg}")
        import traceback
        traceback.print_exc()
        print(f"=== END ERROR ===")
        
        # Return more specific error messages
        if "Could not load audio file" in error_msg:
            return jsonify({'error': 'Unable to load audio file. Please try a different format (WAV, MP3, etc.)'}), 400
        elif "Audio too short" in error_msg:
            return jsonify({'error': 'Audio file is too short. Please upload at least 1 second of audio.'}), 400
        else:
            return jsonify({'error': f'Audio processing failed: {error_msg}'}), 500
    
    finally:
        # Clean up temporary file
        if tmp_file_path and os.path.exists(tmp_file_path):
            try:
                os.unlink(tmp_file_path)
                print(f"Cleaned up temporary file: {tmp_file_path}")
            except Exception as cleanup_error:
                print(f"Warning: Could not clean up temp file: {cleanup_error}")

@app.route('/demo')
def demo():
    """Generate demo results for testing"""
    risk_score = np.random.uniform(10, 90)
    
    # Generate realistic transcript based on risk level
    transcript = analyzer.generate_realistic_transcript(risk_score)
    
    # Generate mock features
    features = {name: np.random.uniform(0, 1) for name in analyzer.feature_names}
    
    # Adjust features to match risk level
    if risk_score > 60:
        features['speaking_rate'] = np.random.uniform(60, 100)
        features['pause_rate'] = np.random.uniform(0.3, 0.6)
        features['type_token_ratio'] = np.random.uniform(0.2, 0.4)
    else:
        features['speaking_rate'] = np.random.uniform(100, 150)
        features['pause_rate'] = np.random.uniform(0.1, 0.3)
        features['type_token_ratio'] = np.random.uniform(0.4, 0.7)
    
    risk_assessment = analyzer._calculate_risk_assessment(risk_score, features)
    biomarkers = analyzer._generate_biomarker_report(features, risk_score)
    
    # Convert numpy types to Python native types for JSON serialization
    def convert_numpy_types(obj):
        if isinstance(obj, dict):
            return {key: convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj
    
    demo_results = {
        'transcript': transcript,
        'features': features,
        'risk_score': risk_score,
        'risk_assessment': risk_assessment,
        'biomarkers': biomarkers
    }
    
    return jsonify(convert_numpy_types(demo_results))

# Add route to serve saved recordings
@app.route('/recordings/<filename>')
def serve_recording(filename):
    """Serve saved recording files"""
    recordings_dir = os.path.join(os.getcwd(), 'recordings')
    return send_from_directory(recordings_dir, filename)

# Add static folder for images
app.static_folder = 'static'

if __name__ == '__main__':
    import sys
    port = 5001
    if len(sys.argv) > 1 and sys.argv[1] == '--port':
        port = int(sys.argv[2])
    app.run(debug=True, host='0.0.0.0', port=port)
