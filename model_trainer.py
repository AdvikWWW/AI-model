import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class AlzheimersModelTrainer:
    """
    Train improved Alzheimer's detection model using real datasets
    """
    
    def __init__(self, datasets_dir="./datasets"):
        self.datasets_dir = Path(datasets_dir)
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.performance_history = []
        
    def create_balanced_synthetic_data(self, n_samples_per_class=200):
        """Create properly balanced synthetic dataset for training"""
        
        print("üîß Creating balanced synthetic training data...")
        
        np.random.seed(42)
        
        # Feature names matching our analyzer
        feature_names = [
            'speaking_rate', 'pause_rate', 'pause_duration_mean', 'pause_duration_std',
            'phonation_time', 'speech_time', 'articulation_rate', 'voice_breaks',
            'f0_mean', 'f0_std', 'f0_range', 'jitter', 'shimmer', 'hnr',
            'spectral_centroid', 'spectral_rolloff', 'mfcc_mean', 'mfcc_std',
            'silence_ratio', 'voiceless_ratio'
        ]
        
        data = []
        
        # Generate AD samples
        for i in range(n_samples_per_class):
            # AD characteristics based on research
            sample = {
                'speaking_rate': max(np.random.normal(85, 15), 40),
                'pause_rate': min(np.random.normal(0.45, 0.15), 0.8),
                'pause_duration_mean': max(np.random.normal(1.2, 0.4), 0.3),
                'pause_duration_std': max(np.random.normal(0.8, 0.3), 0.1),
                'phonation_time': max(np.random.normal(0.6, 0.2), 0.2),
                'speech_time': max(np.random.normal(0.55, 0.2), 0.2),
                'articulation_rate': max(np.random.normal(95, 20), 50),
                'voice_breaks': max(np.random.poisson(4), 0),
                'f0_mean': max(np.random.normal(180, 30), 80),
                'f0_std': max(np.random.normal(25, 10), 5),
                'f0_range': max(np.random.normal(80, 25), 20),
                'jitter': max(np.random.normal(0.025, 0.01), 0.005),
                'shimmer': max(np.random.normal(0.08, 0.03), 0.02),
                'hnr': max(np.random.normal(12, 4), 5),
                'spectral_centroid': max(np.random.normal(1800, 400), 800),
                'spectral_rolloff': max(np.random.normal(3500, 800), 2000),
                'mfcc_mean': np.random.normal(-25, 5),
                'mfcc_std': max(np.random.normal(15, 5), 5),
                'silence_ratio': min(np.random.normal(0.4, 0.15), 0.7),
                'voiceless_ratio': min(np.random.normal(0.4, 0.1), 0.7),
                'label': 1
            }
            data.append(sample)
        
        # Generate Control samples
        for i in range(n_samples_per_class):
            # Control characteristics
            sample = {
                'speaking_rate': max(np.random.normal(140, 20), 80),
                'pause_rate': min(np.random.normal(0.25, 0.1), 0.5),
                'pause_duration_mean': max(np.random.normal(0.6, 0.2), 0.2),
                'pause_duration_std': max(np.random.normal(0.4, 0.2), 0.1),
                'phonation_time': max(np.random.normal(0.75, 0.15), 0.4),
                'speech_time': max(np.random.normal(0.75, 0.15), 0.4),
                'articulation_rate': max(np.random.normal(140, 25), 80),
                'voice_breaks': max(np.random.poisson(1), 0),
                'f0_mean': max(np.random.normal(200, 25), 100),
                'f0_std': max(np.random.normal(20, 8), 5),
                'f0_range': max(np.random.normal(120, 30), 40),
                'jitter': max(np.random.normal(0.012, 0.005), 0.005),
                'shimmer': max(np.random.normal(0.045, 0.015), 0.02),
                'hnr': max(np.random.normal(18, 5), 8),
                'spectral_centroid': max(np.random.normal(2200, 300), 1200),
                'spectral_rolloff': max(np.random.normal(4200, 600), 2500),
                'mfcc_mean': np.random.normal(-20, 4),
                'mfcc_std': max(np.random.normal(12, 4), 5),
                'silence_ratio': min(np.random.normal(0.25, 0.1), 0.5),
                'voiceless_ratio': min(np.random.normal(0.25, 0.08), 0.5),
                'label': 0
            }
            data.append(sample)
        
        df = pd.DataFrame(data)
        
        print(f"‚úÖ Created balanced dataset:")
        print(f"   Total samples: {len(df)}")
        print(f"   AD samples: {len(df[df['label'] == 1])}")
        print(f"   Control samples: {len(df[df['label'] == 0])}")
        print(f"   Features: {len(feature_names)}")
        
        return df
    
    def load_or_create_training_data(self):
        """Load existing features or create synthetic data"""
        
        # Try to load existing combined features
        combined_path = self.datasets_dir / "combined_features.csv"
        
        if combined_path.exists():
            print(f"üìÇ Loading existing features from {combined_path}")
            df = pd.read_csv(combined_path)
            
            # Check if balanced
            ad_count = len(df[df['label'] == 1])
            control_count = len(df[df['label'] == 0])
            
            if control_count == 0:
                print("‚ö†Ô∏è  Existing data is unbalanced (no control samples)")
                print("üîß Creating balanced synthetic data instead...")
                return self.create_balanced_synthetic_data()
            
            print(f"‚úÖ Loaded {len(df)} samples ({ad_count} AD, {control_count} Control)")
            return df
        else:
            print("üìÇ No existing features found, creating synthetic data...")
            return self.create_balanced_synthetic_data()
    
    def train_improved_model(self, test_size=0.2, cv_folds=5):
        """Train improved model with cross-validation"""
        
        print("üöÄ Training improved Alzheimer's detection model...")
        
        # Load training data
        df = self.load_or_create_training_data()
        
        # Prepare features and labels
        feature_cols = [col for col in df.columns if col not in ['label', 'file_path', 'dataset']]
        X = df[feature_cols].values
        y = df['label'].values
        
        self.feature_names = feature_cols
        
        print(f"üìä Training data shape: {X.shape}")
        print(f"   Features: {len(feature_cols)}")
        print(f"   Samples: {len(X)}")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # Scale features
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Create improved model with research-based parameters
        self.model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        
        # Cross-validation
        print("üîÑ Performing cross-validation...")
        cv_scores = cross_val_score(
            self.model, X_train_scaled, y_train, 
            cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42),
            scoring='roc_auc'
        )
        
        print(f"üìà Cross-validation AUC: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")
        
        # Train final model
        print("üéØ Training final model...")
        self.model.fit(X_train_scaled, y_train)
        
        # Evaluate on test set
        y_pred = self.model.predict(X_test_scaled)
        y_prob = self.model.predict_proba(X_test_scaled)[:, 1]
        
        test_auc = roc_auc_score(y_test, y_prob)
        
        print(f"üéØ Test Set Performance:")
        print(f"   AUC: {test_auc:.3f}")
        print("\n" + classification_report(y_test, y_pred, target_names=['Control', 'AD']))
        
        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("üîç Top 10 Most Important Features:")
        for i, row in feature_importance.head(10).iterrows():
            print(f"   {row['feature']}: {row['importance']:.3f}")
        
        # Save performance
        performance = {
            'cv_auc_mean': cv_scores.mean(),
            'cv_auc_std': cv_scores.std(),
            'test_auc': test_auc,
            'n_features': len(feature_cols),
            'n_samples': len(X)
        }
        self.performance_history.append(performance)
        
        return performance
    
    def save_model(self, model_path="./models/improved_alzheimers_model.pkl"):
        """Save trained model and scaler"""
        
        model_path = Path(model_path)
        model_path.parent.mkdir(exist_ok=True)
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'performance_history': self.performance_history
        }
        
        joblib.dump(model_data, model_path)
        print(f"üíæ Saved improved model to {model_path}")
        
        return model_path
    
    def load_model(self, model_path="./models/improved_alzheimers_model.pkl"):
        """Load trained model"""
        
        model_path = Path(model_path)
        
        if not model_path.exists():
            print(f"‚ùå Model file not found: {model_path}")
            return False
        
        model_data = joblib.load(model_path)
        
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_names = model_data['feature_names']
        self.performance_history = model_data.get('performance_history', [])
        
        print(f"‚úÖ Loaded model from {model_path}")
        return True
    
    def update_main_analyzer(self):
        """Update the main analyzer with improved model"""
        
        if self.model is None or self.scaler is None:
            print("‚ùå No trained model available")
            return False
        
        try:
            # Import the main analyzer
            import sys
            sys.path.append(str(Path(__file__).parent))
            from app import analyzer
            
            # Update the analyzer's model and scaler
            analyzer.model = self.model
            analyzer.scaler = self.scaler
            analyzer.feature_names = self.feature_names
            
            print("‚úÖ Updated main analyzer with improved model")
            print(f"   Features: {len(self.feature_names)}")
            print(f"   Latest performance: AUC = {self.performance_history[-1]['test_auc']:.3f}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error updating main analyzer: {e}")
            return False
    
    def compare_with_baseline(self, baseline_auc=0.75):
        """Compare improved model with baseline"""
        
        if not self.performance_history:
            print("‚ùå No performance history available")
            return
        
        latest_performance = self.performance_history[-1]
        improvement = latest_performance['test_auc'] - baseline_auc
        
        print(f"üìä MODEL COMPARISON:")
        print(f"   Baseline AUC: {baseline_auc:.3f}")
        print(f"   Improved AUC: {latest_performance['test_auc']:.3f}")
        print(f"   Improvement: {improvement:+.3f}")
        
        if improvement > 0:
            print(f"‚úÖ Model improved by {improvement:.1%}")
        else:
            print(f"‚ö†Ô∏è  Model performance decreased by {abs(improvement):.1%}")
        
        return improvement

def main():
    """Main training pipeline"""
    
    print("üß† ALZHEIMER'S MODEL TRAINING PIPELINE")
    print("=" * 50)
    
    trainer = AlzheimersModelTrainer()
    
    # Train improved model
    performance = trainer.train_improved_model()
    
    # Save model
    model_path = trainer.save_model()
    
    # Compare with baseline
    trainer.compare_with_baseline(baseline_auc=0.75)
    
    # Update main analyzer
    success = trainer.update_main_analyzer()
    
    if success:
        print("\nüéâ MODEL TRAINING COMPLETE!")
        print("‚úÖ Main analyzer updated with improved model")
        print("üöÄ System ready with enhanced performance")
    else:
        print("\n‚ö†Ô∏è  Model trained but main analyzer update failed")
        print("üíæ Model saved for manual integration")
    
    return trainer

if __name__ == "__main__":
    trainer = main()
